{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(64000, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): GELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Initialize Variables\n",
    "model_name = \"AI-Sweden-Models/gpt-sw3-126m\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "prompt = \"Träd är fina för att\"\n",
    "\n",
    "# Initialize Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1D(nf=3072, nx=768)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.h[0].mlp.c_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Stålproduktionen i USA er stærkt politiseret. National sikkerhed nævnes ofte som en grund til at beskytte industrien mod udenlandsk konkurrence. USA,\"\n",
    "generator = pipeline('text-generation', tokenizer=tokenizer, model=model, device=device)\n",
    "generated = generator(prompt, max_new_tokens=250, do_sample=True, temperature=0.6, top_p=1)[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stålproduktionen i USA er stærkt politiseret. National sikkerhed nævnes ofte som en grund til at beskytte industrien mod udenlandsk konkurrence. USA, der har været en af de mest magtfulde industrilande i verden, er med til at beskytte industrien i USA.\\nUSA har været et af de mest magtfulde industrilande i verden i de næste 40 år. De har i de senere år udviklet en række produkter, som blandt andet er blevet udstillet på verdens største udstilling i Paris. I 2016 blev USA kåret til \"Vigtigst for USA\" af \"Videnskab.dk\" og blev tildelt den prestigefyldte amerikanske \"International Business Journal\" - verdens bedste business-journalist.\\nUSA har en tradition for at være en foregangsland inden for teknologisk innovation, når det gælder forskning og udvikling. Det er en af årsagerne til, at USA har været blandt de mest innovative og dynamiske industrilande i verden. USA har i dag et af verdens største forskningsmiljøer og en ledende position inden for teknologisk innovation.\\nUSA er et af de mest dynamiske industrilande i verden. I 2016 blev USA kåret til \"Vigtigst for USA\" af \"Videnskab.dk\" og blev tildelt den prestigefyldte amerikanske \"International Business Journal\" - verdens bedste business-journalist.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_probes import HookManager, TextClassificationDataset, ProbeTrainer\n",
    "\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch import nn\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "from classification_probes import TextClassificationDataset, ProbeTrainer, HookManager, ClassificationProbe\n",
    "\n",
    "try:\n",
    "    hidden_size = model.config.n_embd\n",
    "except AttributeError:\n",
    "    hidden_size = model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"AI-Sweden-Models/gpt-sw3-126m\",\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"apply_query_key_layer_scaling\": true,\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 3,\n",
       "  \"initializer_range\": 0.023,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 2048,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": 3072,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 2048,\n",
       "  \"normalize_attention_scores\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"tokenizer_class\": \"GPTSw3Tokenizer\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 64000\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_map = {\n",
    "    'da': 0,\n",
    "    'en': 1,\n",
    "    'is': 2,\n",
    "    'nb': 3,\n",
    "    'sv': 4\n",
    "}\n",
    "\n",
    "data_loc = 'data/antibiotic/'\n",
    "ds = TextClassificationDataset.from_txt(data_loc + 'da.txt', lab_map['da'])\n",
    "ds.add_from_txt(data_loc + 'en.txt', lab_map['en'])\n",
    "#ds.add_from_txt(data_loc + 'is.txt', lab_map['is'])\n",
    "#ds.add_from_txt(data_loc + 'nb.txt', lab_map['nb'])\n",
    "#ds.add_from_txt(data_loc + 'sv.txt', lab_map['sv'])\n",
    "\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "activation_ds = ActivationDataset()\n",
    "\n",
    "for text, label in loader:\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "\n",
    "    with HookManager(model) as hook_manager:\n",
    "        res_stream_act = hook_manager.attach_residstream_hook(\n",
    "            layer=2,\n",
    "            pre_mlp=False,\n",
    "            pythia=False\n",
    "        )\n",
    "\n",
    "        model(**tokenized)\n",
    "\n",
    "    # flattening [batch, pad_size, ...] to [tokens, ...]\n",
    "    attn_mask = tokenized.attention_mask.flatten() # [tokens]\n",
    "    label = label.unsqueeze(-1).expand(-1, tokenized.attention_mask.shape[1]).flatten() # [tokens]\n",
    "    res_stream_act = res_stream_act[0].view(-1, hidden_size) # [tokens, hidden_size]\n",
    "\n",
    "    activation_ds.add_with_mask(res_stream_act, label, attn_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45316"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activation_ds.acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.8405e-02, -6.8613e-03, -3.9146e-02,  5.6455e-03, -1.9041e-02,\n",
       "         3.8255e-02,  8.9936e-03,  3.7906e-02,  1.4006e-02,  4.4132e-03,\n",
       "         1.6004e-02,  1.5326e-02,  5.3193e-02,  4.2222e-02,  1.2184e-02,\n",
       "         1.3802e-02,  5.6083e-03,  4.1842e-03,  1.8618e-02,  5.6741e-02,\n",
       "        -3.6182e-02,  2.4139e-02, -9.3439e-03,  3.1805e-02, -2.8023e-02,\n",
       "        -3.1447e-02,  3.4404e-02,  1.4710e-02,  1.1558e-02, -9.1331e-02,\n",
       "        -4.7984e-03,  1.2365e-03,  6.7118e-02,  3.3524e-03,  1.1171e-01,\n",
       "         7.5385e-02, -1.5727e-02,  3.8353e-02, -1.1505e-02,  2.2298e-02,\n",
       "        -3.5171e-03,  6.8540e-02, -1.1583e-02,  3.7033e-02, -3.8669e-03,\n",
       "        -3.0202e-02,  6.5236e-02, -1.6729e-02,  2.2713e-02,  2.2648e-02,\n",
       "        -3.9186e-02,  2.7850e-02,  5.6906e-02,  5.4721e-02,  4.0120e-02,\n",
       "        -6.5567e-03,  4.4506e-02,  7.4212e-02,  1.5662e-03,  2.1036e-02,\n",
       "        -2.3505e-02,  1.4239e-02,  1.9211e-02,  1.0068e-01,  7.6835e-02,\n",
       "         5.1052e-02,  5.2480e-02,  1.6218e-03,  4.2987e-02,  2.3418e-02,\n",
       "         1.6593e-02,  2.3311e-02,  6.0059e-02,  3.4329e-02,  3.1997e-02,\n",
       "        -2.2374e-04,  6.7031e-02, -4.9093e-02,  1.6152e-02,  8.2480e-02,\n",
       "        -4.3333e-02,  4.9509e-02, -1.6911e-02,  6.3166e-02,  5.7378e-03,\n",
       "         3.1061e-02, -2.8888e-02, -2.5197e-02, -1.2391e-02,  6.8238e-02,\n",
       "         1.5836e-02,  7.0721e-02, -1.0375e-02,  4.6759e-02,  2.0407e-01,\n",
       "         3.7368e-02,  1.1241e-02,  5.1252e-02, -2.9529e-02,  4.3293e-02,\n",
       "         2.5341e-02,  1.1612e-02,  1.1792e-02,  5.3088e-02,  2.1654e-02,\n",
       "         1.2570e-02, -2.9598e-02,  1.1224e-02,  7.5529e-02,  4.7321e-02,\n",
       "        -1.4593e-02, -1.0589e-02,  4.7281e-02, -1.8705e-03, -1.8481e-02,\n",
       "         3.9315e-02,  2.0847e-02, -3.3783e-02, -1.3353e-03,  5.5540e-02,\n",
       "         6.3730e-02,  4.5474e-02,  1.7803e-02, -2.3447e-02,  7.9890e-02,\n",
       "         2.7813e-02,  1.0007e-01,  7.7799e-02, -1.5863e-01,  2.9331e-02,\n",
       "         5.9046e-02,  8.1031e-02,  3.1732e-02,  6.0330e-01,  1.1406e-01,\n",
       "        -1.1989e-02,  7.0223e-02, -1.3692e-02, -5.1081e-01, -8.1020e-03,\n",
       "         2.6917e-02,  3.0731e-02,  5.1801e-02, -3.0640e-02,  7.7002e-02,\n",
       "         6.8606e-01,  6.1474e-02,  3.2736e-02, -6.5639e-02, -5.2974e-02,\n",
       "         1.0877e-01, -1.0100e-02,  7.2688e-02,  1.0133e+00,  6.0293e-02,\n",
       "         9.2170e-02,  6.6856e-03,  5.5129e-03,  6.5286e-02,  2.3789e-02,\n",
       "         4.7396e-02,  4.2642e-02,  4.6478e-02,  5.0248e-02,  2.5203e-02,\n",
       "         5.1013e-02, -1.4847e-02, -1.1141e-02,  9.6555e-03,  3.9747e-03,\n",
       "         1.9895e-02,  4.8722e-02,  3.1745e-02, -1.2157e-02,  5.6992e-02,\n",
       "         3.0744e-02,  2.5420e-02,  9.1271e-03,  4.1215e-02,  7.9115e-03,\n",
       "         3.6927e-02,  6.9347e-02, -1.7921e-02,  4.6974e-02,  8.0209e-03,\n",
       "        -6.7666e-02,  3.0086e-02,  4.3620e-02,  2.4802e-01,  5.3710e-02,\n",
       "         1.7769e-02,  5.9574e-02,  3.6106e-02, -1.0150e-02,  6.7156e-02,\n",
       "        -5.8102e-02,  3.8683e-02,  2.2500e-02,  4.9054e-02,  2.0711e-02,\n",
       "         6.7484e-03, -9.6858e-06,  9.2091e-02, -3.2336e-02, -2.1420e-02,\n",
       "         2.9789e-02, -1.4809e-02,  7.8665e-02,  8.1051e-03,  2.6433e-02,\n",
       "        -1.8221e-02,  8.5155e-02, -3.7595e-02,  1.7259e-02,  1.3384e-02,\n",
       "         1.1545e-01, -2.5053e-02,  8.6929e-02,  1.1591e-02,  3.7676e-02,\n",
       "         2.6787e-02,  7.5879e-02, -4.2191e-02,  1.5315e-02,  5.1495e-02,\n",
       "         3.7404e-02,  6.1849e-02,  4.2245e-02,  6.4016e-02, -1.9921e-02,\n",
       "        -1.1271e-02,  5.3559e-02, -2.2186e-03,  1.3635e-01,  2.6954e-02,\n",
       "         4.4362e-02, -2.8807e-02,  5.3558e-02,  1.3661e-02,  1.2152e-01,\n",
       "        -1.7829e-02,  6.3884e-02,  4.3342e-02,  4.9530e-02,  2.9165e-02,\n",
       "        -4.4410e-03, -1.2705e-02,  1.1910e-01, -2.9080e-02,  6.3415e-02,\n",
       "         1.2663e-02,  4.7661e-02,  3.2308e-02,  5.0861e-02,  2.7239e-02,\n",
       "        -1.8498e-04,  6.2566e-02,  3.8867e-02, -2.7309e-02,  3.0197e-02,\n",
       "        -6.8787e-03, -2.2987e-02,  1.4278e-03,  3.2522e-02,  9.2758e-03,\n",
       "        -1.8312e-03,  2.9099e-02, -9.3425e-03,  1.6892e-02,  3.1712e-02,\n",
       "         6.8740e-02,  1.6028e-02,  6.0419e-02,  5.6286e-03,  4.8943e-03,\n",
       "        -2.5321e-02, -1.4209e-02,  3.0771e-02, -4.6436e-02,  3.4578e-02,\n",
       "         1.9569e-02,  4.0112e-02,  9.6107e-02,  3.4932e-02,  1.0225e-02,\n",
       "         3.7339e-02,  1.2300e-02, -3.8029e-02,  3.3062e-02,  1.4678e-02,\n",
       "        -3.3928e-02, -1.4526e-02,  3.5523e-02,  2.1017e-02,  2.2810e-02,\n",
       "         9.0867e-02, -7.4127e-03,  1.3077e-02,  3.3777e-02, -4.3893e-02,\n",
       "         6.0032e-03,  2.1265e-02, -1.7245e-02,  2.5573e-02,  4.9125e-02,\n",
       "         3.7368e-02,  4.6246e-02,  1.8021e-02,  2.7826e-02,  2.6618e-02,\n",
       "        -2.4768e+00,  5.5705e-03, -7.4782e-03,  2.9695e-02,  5.4381e-02,\n",
       "        -6.0152e-03,  8.5310e-02,  7.3030e-02,  6.5656e-02,  5.2678e-02,\n",
       "        -3.0354e-02,  2.2806e-02,  7.9751e-03,  6.3576e-02,  2.7173e-02,\n",
       "         3.1763e-02,  3.4129e-02,  3.2160e-02, -2.5568e-02,  9.1112e-03,\n",
       "        -1.8728e-02, -1.7317e-02,  1.4198e-02,  3.6515e-02,  5.0214e-02,\n",
       "         5.2868e-02,  3.2725e-02,  3.5408e-02,  7.7163e-02,  7.9082e-02,\n",
       "         5.5816e-02, -7.8067e-03,  2.4836e-02,  8.2165e-02,  2.3684e-02,\n",
       "        -1.4709e-02,  3.2206e-02,  1.2468e-02,  2.2369e-02,  1.0298e-02,\n",
       "         2.3674e-02, -5.3203e-02,  5.8099e-02,  4.1350e-03,  2.2506e-02,\n",
       "         1.7372e-02, -3.4192e-03,  3.7216e-02,  4.8669e-03,  1.5698e-02,\n",
       "         3.9972e-02,  3.0854e-02, -1.0926e-02,  3.7568e-02,  2.3521e+00,\n",
       "         7.2698e-02, -3.6388e-02,  3.5479e-02, -7.0435e-03,  2.1944e-02,\n",
       "         2.2313e-02,  5.2970e-02,  1.3334e-01,  2.5739e-03,  7.2560e-02,\n",
       "         3.5859e-02, -1.9181e-02,  7.5449e-02, -2.9176e-02,  6.8576e-03,\n",
       "         4.9176e-02, -8.7787e-02,  3.2429e-02,  9.3025e-02,  3.5403e-02,\n",
       "         2.1688e-02, -6.3942e-02,  5.1705e-02,  7.1373e-03,  5.1511e-02,\n",
       "         6.0447e-02,  1.3453e-02,  4.7538e-02,  3.5430e-02, -1.5176e-02,\n",
       "        -4.0564e-03,  5.2368e-02,  7.4369e-02, -9.1837e-03,  6.2387e-03,\n",
       "         5.9126e-02,  6.8185e-02, -1.1387e-02,  6.1938e-02,  3.6367e-02,\n",
       "        -8.1800e-03,  2.7265e-02,  6.1508e-03, -6.2318e-03, -7.5345e-03,\n",
       "         2.4893e-02, -5.3295e-02,  4.4232e-02,  1.0213e-02, -8.0272e-03,\n",
       "         3.1253e-02,  1.5867e-02,  4.3493e-03, -2.9820e-03,  3.4797e-02,\n",
       "         4.7724e-02,  3.8111e-02,  3.0159e-02,  1.0285e-01,  3.5902e-03,\n",
       "        -1.9910e-03, -9.8076e-03,  4.8123e-03, -2.0826e-02,  1.2345e-01,\n",
       "         6.8949e-03,  4.0613e-02,  4.8575e-02,  1.8391e-02, -2.1895e-03,\n",
       "         6.6393e-03,  1.0517e-02,  3.3369e-02,  9.3085e-02,  2.9850e-02,\n",
       "         2.8534e-02,  3.6117e-02,  4.5632e-02,  7.3057e-02, -1.0480e-02,\n",
       "         5.2112e-02,  2.2584e-02,  6.4341e-02,  2.7671e-02,  5.1640e-02,\n",
       "        -2.5364e-02,  8.4170e-02,  7.7791e-02,  4.3012e-02,  3.8434e-02,\n",
       "        -2.8195e-03,  4.8518e-02,  6.4223e-02,  7.2057e-02,  1.1306e-02,\n",
       "         4.1102e-02,  1.4433e-02,  1.0368e-02,  2.4510e-02,  2.4321e-02,\n",
       "         1.4024e-02,  3.2499e-02,  2.8076e-02, -2.3269e-03,  9.4196e-03,\n",
       "         3.5311e-02,  2.7152e-02,  1.0370e-01, -2.1101e+01,  1.1307e-02,\n",
       "         4.6521e-02,  4.0838e-02,  4.3144e-02,  2.7546e-02,  3.4345e-02,\n",
       "        -2.2266e-02, -2.1427e-03,  1.3595e+00,  3.0119e-03,  4.2617e-02,\n",
       "         1.4640e-04, -1.2020e-02,  6.5181e-02,  3.7271e-02,  6.1554e-03,\n",
       "        -1.6866e-02,  4.2647e-02,  4.0310e-02,  8.9598e-03, -1.0583e-02,\n",
       "         1.1726e-02, -4.1928e-02,  7.3977e-02,  1.7761e-02,  8.4630e-03,\n",
       "         2.6021e-02,  4.0590e-02,  3.0283e-02,  6.3481e-02,  2.1931e-02,\n",
       "         2.0930e-02,  4.8583e-02,  6.6767e-02, -1.4918e-02,  7.2831e-02,\n",
       "         6.5459e-02,  1.4120e-02, -1.5194e-02, -1.2886e-03,  5.8013e-02,\n",
       "         9.0532e-03, -2.3004e-03,  7.6021e-02,  5.7841e-02,  8.1179e-02,\n",
       "         1.4973e-02,  1.1128e+00,  9.1571e-02, -1.1634e-02,  2.6245e-02,\n",
       "         4.7986e-02,  2.3004e-02,  8.2084e-03,  2.3969e-02,  2.6588e-02,\n",
       "         4.8459e-02,  6.9534e-02,  1.9270e-02,  1.4208e-02,  1.1975e-02,\n",
       "        -3.3649e-04,  5.9942e-03,  3.2520e-02,  1.7601e-02, -2.2355e-03,\n",
       "         5.7951e-02,  6.9845e-03, -1.7029e-02,  7.7502e-02,  2.3263e-02,\n",
       "         5.2468e-02, -1.3489e-02, -2.1913e-02, -6.0674e-02,  3.1003e-02,\n",
       "         1.4100e-01,  4.7321e-02,  3.0552e-02,  2.8712e-02,  1.1485e-01,\n",
       "         1.2675e-02, -7.4114e-03,  5.8827e-02,  5.1751e-02,  2.8503e-02,\n",
       "         2.3390e-02,  6.6414e-03, -3.6425e-02,  7.9540e-02, -2.2733e-02,\n",
       "         3.0125e-02,  7.4564e-02,  1.8480e-02,  4.9365e-02, -2.2378e-04,\n",
       "         6.7081e-02, -2.6435e-02, -5.7729e-02, -2.2287e-02,  5.4287e-02,\n",
       "         1.0523e-01,  5.0384e-04,  3.1453e-02,  3.2407e-02,  1.0353e-02,\n",
       "         1.2920e-02,  8.0880e-03,  6.1731e-02,  4.2867e-02,  2.6490e-02,\n",
       "         4.4158e-02,  2.5147e-02,  3.8286e-02,  4.3321e-02, -2.7211e-02,\n",
       "         3.4383e-02,  6.5946e-02, -1.1373e-02, -1.1684e-02,  7.3533e-02,\n",
       "         8.9543e-02,  7.1832e-02, -1.2701e-02,  5.0348e-02, -4.4419e-02,\n",
       "         1.7985e-02, -1.1544e-01,  1.3207e-02,  7.5930e-02, -1.4487e-02,\n",
       "         6.3673e-02, -3.2714e-03,  7.6533e-02,  1.4488e-02, -1.8989e-02,\n",
       "         1.8408e-02,  2.9607e-02,  8.7346e-02,  5.6571e-02, -3.4723e-02,\n",
       "         3.9149e-02,  4.1025e-02,  3.2729e-02,  3.3036e-02,  7.2125e-02,\n",
       "        -4.4921e-02, -5.6816e-04, -9.9419e-03,  1.4447e-02,  1.1434e-01,\n",
       "        -1.9003e-02,  2.1338e-02, -4.9643e-02,  1.3217e-02,  2.4710e-02,\n",
       "         3.9992e-01,  4.7530e-02,  1.9152e-02,  3.7381e-02, -9.1489e-03,\n",
       "         3.3298e-02,  7.1450e-02,  8.0914e-02, -4.3885e-02,  9.2335e-02,\n",
       "         5.4903e-01,  4.9505e-02,  3.9557e-02,  6.1848e-02,  7.1936e-03,\n",
       "         1.1161e-01,  2.0777e-02,  7.2187e-02,  3.5596e-02,  1.7613e-02,\n",
       "        -4.9005e-02, -8.4355e-03, -1.0043e-02,  2.9736e-02,  1.5530e-02,\n",
       "        -2.0280e-01,  4.9311e-03, -6.0131e-02,  4.7591e-02,  2.9951e-03,\n",
       "         5.5208e-02,  6.0947e-03,  7.8957e-02,  5.0768e-02,  1.8553e-02,\n",
       "         2.0004e-02,  1.9384e-02, -1.0322e-01,  7.8040e-02,  4.0039e-04,\n",
       "        -2.8224e-02,  8.2065e-02, -1.0816e-02, -2.9733e-02, -2.1476e-02,\n",
       "        -4.4694e-03, -1.2520e+00,  6.8728e-02, -4.1731e-03,  4.5942e-02,\n",
       "         9.1022e-03,  8.1983e-02, -2.0138e-03,  4.2951e-02,  7.3994e-02,\n",
       "         9.5798e-02,  6.1171e-04,  3.2158e-02,  3.6241e-02,  1.7363e-02,\n",
       "         7.6112e-02, -2.9509e-02,  4.6308e-03,  1.9943e-02, -5.9375e-02,\n",
       "         3.7049e-02,  3.1195e-02,  2.7784e-03,  3.9022e-02,  5.4794e-02,\n",
       "         6.3595e-02,  2.5876e-02,  3.2851e-02,  2.7130e-02,  2.9501e-02,\n",
       "         5.3576e-02, -1.4635e-02,  2.1012e-02,  6.0335e-02, -6.1295e-03,\n",
       "         3.3495e-02,  2.5544e-02,  3.2740e-02, -5.1803e-03,  2.5243e-02,\n",
       "         2.8021e-02, -1.0127e-02,  3.9317e-02,  2.5443e-02,  7.8403e-02,\n",
       "         1.1475e-02,  5.7771e-02,  1.1147e+00,  8.5712e-02, -1.3250e-02,\n",
       "        -8.3178e-03,  5.4350e-02,  5.4306e-02,  1.0805e-03,  4.2899e-02,\n",
       "         3.9144e-02,  2.6467e-02,  1.3159e-01,  1.8197e-03,  3.6685e-02,\n",
       "         1.5282e-02, -5.2064e-03, -6.6849e-02,  8.8437e-03,  3.3267e-02,\n",
       "         4.2861e-02,  3.9108e-02,  1.7554e-02,  3.3228e-02,  5.0663e-02,\n",
       "        -1.4795e-02,  3.8377e-02,  3.5442e-02, -1.8922e+00, -3.0941e-02,\n",
       "         3.1127e-02,  5.0178e-02, -6.4431e-03, -1.3925e-02,  2.9777e-02,\n",
       "         1.2825e-02,  3.3148e-03, -7.8465e-02,  2.7566e-02, -1.6912e-01,\n",
       "         8.0636e-02, -2.4853e-02, -2.8969e-02,  5.9843e-02, -4.3636e-02,\n",
       "         4.5473e-03,  3.6275e-02,  6.1950e-02])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_ds.acts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_ds.labels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why can we subtract a feature but not aplify a feature\n",
    "#talk about other steering method https://arxiv.org/html/2402.01618v1#S3.SS1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "danish = ([tensor for tensor, label in activation_ds if label == activation_ds.labels[lab_map[\"da\"]]])\n",
    "english = ([tensor for tensor, label in activation_ds if label == activation_ds.labels[lab_map[\"en\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "danish_steering = torch.stack(danish).mean(dim=0)\n",
    "english_steering = torch.stack(english).mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a short story about 500000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "This is a short story about 6 stories, that's 14 stories. 4   5   6                                          \n",
      "\n",
      "This is a short story about 5 min, so 5 min er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er er\n",
      "\n",
      "This is a short story about 100000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "This is a short story about 200000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "This is a short story about 100000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "This is a short story about 30 stories and 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories, 40 stories\n",
      "\n",
      "This is a short story about 100000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "This is a short story about 100000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "This is a short story about 12 stories,                                                           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = 'This is a short story about '\n",
    "tokenized = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "with HookManager(model) as hook_manager:\n",
    "    # hook_manager.attach_resid_stream_steer_hook(\n",
    "    #     6,\n",
    "    #     danish_steering,\n",
    "    #     2,\n",
    "    #     pre_mlp=False,\n",
    "    #     pythia=False\n",
    "    # )\n",
    "    hook_manager.attach_resid_stream_steer_hook(\n",
    "        6,\n",
    "        english_steering,\n",
    "        -2,\n",
    "        pre_mlp=False,\n",
    "        pythia=False\n",
    "    )\n",
    "\n",
    "\n",
    "    output_nb_steering = [\n",
    "        model.generate(tokenized.input_ids, max_length=70, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "        for _ in range(10)\n",
    "    ]\n",
    "\n",
    "for output in output_nb_steering:\n",
    "    print(tokenizer.decode(output[0]).replace('\\n', ' '))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
