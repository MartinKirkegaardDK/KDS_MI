{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'da': ('I Begyndelsen skabte Gud Himmelen og Jorden.',\n",
       "  'Og Jorden var øde og tom, og der var Mørke over Verdensdybet. Men Guds Ånd svævede over Vandene.'),\n",
       " 'en': ('In the beginning God created the heavens and the earth.',\n",
       "  \"Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\")}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dansk bibel                            \"bla bla (originalt danski)\" \n",
    "#engelsk bibel \"bla bla\" steering homie \"bla bla (men dansk)\"\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils.steering import generate_with_steering\n",
    "import fasttext\n",
    "import torch\n",
    "import os\n",
    "from classes.datahandling import ParallelNSPDataset\n",
    "from utils.probe_confidence_intervals import model_setup\n",
    "model, tokenizer, device = model_setup(\"AI-Sweden-Models/gpt-sw3-356m\")\n",
    "\n",
    "model_name = \"nb-nordic-lid.ftz\"\n",
    "language_prediction_model = fasttext.load_model(hf_hub_download(\"NbAiLab/nb-nordic-lid\", model_name))\n",
    "\n",
    "label, score = language_prediction_model.predict(\"Harry Potter är en serie fantasyromaner av författaren J.K. Rowling, som började ges\", threshold=0.25)\n",
    "label[0].split(\"__\")[-1], score\n",
    "bible_data = ParallelNSPDataset.from_tmx(\"data/bible-da-en.tmx\",\"da\",\"en\")\n",
    "bible_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/267bnxr16cq2xpr1crtpxbgw0000gn/T/ipykernel_5143/835105919.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  complement[int(layer)] = torch.load(str(steering_vector_path + vector))\n",
      "/var/folders/p0/267bnxr16cq2xpr1crtpxbgw0000gn/T/ipykernel_5143/835105919.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target[int(layer)] = torch.load(str(steering_vector_path +vector))\n",
      "/var/folders/p0/267bnxr16cq2xpr1crtpxbgw0000gn/T/ipykernel_5143/835105919.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined[int(layer)] = torch.load(str(steering_vector_path +vector))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_targeted_steering_vectors(steering_vector_path: str) -> tuple[dict,dict,dict]:\n",
    "    \"\"\"loads steering vectors that are targeted towards a language. \n",
    "    it returns the target, complement and combined, with combined = target - complement\n",
    "\n",
    "    Args:\n",
    "        steering_vector_path (str): some path\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict,dict,dict]: target, complement, combined\n",
    "    \"\"\"\n",
    "    combined = dict()\n",
    "    complement = dict()\n",
    "    target = dict()\n",
    "    for vector in os.listdir(steering_vector_path):\n",
    "        type = vector.split(\"_\")[0]\n",
    "        layer = vector.split(\"_\")[4]\n",
    "        if type == \"combined\":\n",
    "            combined[int(layer)] = torch.load(str(steering_vector_path +vector))\n",
    "        elif type == \"complement\":\n",
    "            complement[int(layer)] = torch.load(str(steering_vector_path + vector))\n",
    "        elif type == \"target\":\n",
    "            target[int(layer)] = torch.load(str(steering_vector_path +vector))\n",
    "    return target, complement, combined\n",
    "\n",
    "steering_vector_path = \"steering_vectors/test_run_2/\"\n",
    "\n",
    "target, complement, combined = load_targeted_steering_vectors(steering_vector_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_outputs(bible_data:dict, layer:int,steering_vector,steering_lambda:int):\n",
    "    danish_prompt = bible_data[50][\"da\"][0].lower()\n",
    "    danish_true_label = bible_data[50][\"da\"][1]\n",
    "    \n",
    "    english_prompt = bible_data[50][\"en\"][0].lower()\n",
    "    english_true_label = bible_data[50][\"en\"][1]\n",
    "    \n",
    "    input_ids = tokenizer(danish_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=30, do_sample=True)[0]\n",
    "    danish_predicted_output = tokenizer.decode(generated_token_ids)[len(danish_prompt):]\n",
    "    \n",
    "    english_predicted_output = generate_with_steering(model,tokenizer,layer,english_prompt,steering_vector[layer], steering_lambda= steering_lambda)\n",
    "    english_predicted_output = english_predicted_output[0][len(english_prompt):]\n",
    "    \n",
    "    return danish_predicted_output, english_predicted_output, danish_true_label,english_true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "danish_predicted_output, english_predicted_output, danish_true_label,english_true_label = gen_outputs(bible_data, 15, combined,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Så lod Gud HERREN Dvale falde over Adam, og da han var sovet ind, tog han et af hans Ribben og lukkede med Kød i dets Sted;'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danish_true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ->\\nThen Adam said to himself, \"I am like a man, who takes up nothing to be, but which would not help the needy'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danish_predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yahweh God caused a deep sleep to fall on the man, and he slept; and he took one of his ribs, and closed up the flesh in its place.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.  20:13 Men da den Herre HERREN, din Gud, gav Navne til alt det, som lever, og til Himmelens fugle og til alle Dyr i Marken, fandt man ikke nogen, der var tilstrækkelig for ham.<|endoftext|>'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
