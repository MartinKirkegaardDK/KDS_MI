{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marki/Documents/GitHub/KDS_MI/utils/create_bible_data.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  complement[int(layer)] = torch.load(str(steering_vector_path + vector),map_location=torch.device(device))\n",
      "/Users/marki/Documents/GitHub/KDS_MI/utils/create_bible_data.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target[int(layer)] = torch.load(str(steering_vector_path +vector),map_location=torch.device(device))\n",
      "/Users/marki/Documents/GitHub/KDS_MI/utils/create_bible_data.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined[int(layer)] = torch.load(str(steering_vector_path +vector),map_location=torch.device(device))\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat (Felis catus), also referred till Ã¤r.\n",
      ". The rest of the world, and the last one, is the rest of the world, and the one\n",
      "F. The last one, and the one, is the last\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from utils.create_bible_data import load_targeted_steering_vectors\n",
    "from utils.steering import generate_with_steering\n",
    "from classes.hook_manager import HookManager\n",
    "from utils.probe_confidence_intervals import model_setup\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block, GPT2Attention, GPT2MLP\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GPT2BlockWithSteering(GPT2Block):\n",
    "    def __init__(self, config, steering_vector: torch.Tensor, scalar: float = 1.0, layer_idx: int = None):\n",
    "        super().__init__(config, layer_idx=layer_idx)\n",
    "        # Prepare steering vector for broadcast: (1, 1, hidden_size)\n",
    "        self.register_buffer('steering_vector', steering_vector.view(1, 1, -1))\n",
    "        self.scalar = scalar\n",
    "\n",
    "    def forward(self, hidden_states: torch.FloatTensor, **kwargs) -> tuple:\n",
    "        # Extract common arguments (compat with huggingface deprecation)\n",
    "        use_cache = kwargs.get('use_cache', False)\n",
    "        output_attn = kwargs.get('output_attentions', False)\n",
    "\n",
    "        # 1. Self-Attention\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.ln_1(hidden_states)\n",
    "        attn_output, self_attn_weights = self.attn(\n",
    "            hidden_states\n",
    "        )\n",
    "        hidden_states = attn_output + residual\n",
    "        \n",
    "\n",
    "        # 3. Feed-Forward with Steering Injection\n",
    "        residual = hidden_states\n",
    "        \n",
    "\n",
    "        hidden_states = self.ln_2(hidden_states)\n",
    "        \n",
    "        hidden_states = hidden_states + self.steering_vector * self.scalar\n",
    "\n",
    "        ff_hidden = self.mlp(hidden_states)\n",
    "        hidden_states = residual + ff_hidden\n",
    "\n",
    "        # 4. Assemble outputs\n",
    "        outputs = (hidden_states,)\n",
    "        if use_cache:\n",
    "            outputs += (self_attn_weights,)\n",
    "        if output_attn:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "#model, tokenizer, device = model_setup(\"downloaded_models/gpt_gptsw3_en_da_is_no_356m_gbs1024\")\n",
    "model, tokenizer, device = model_setup(\"AI-Sweden-Models/gpt-sw3-356m\")\n",
    "\n",
    "\n",
    "target, complement, combined = load_targeted_steering_vectors(\"steering_vectors/DA/\",device)\n",
    "\n",
    "layer_index = 13\n",
    "\n",
    "# Replace layer 5 with your custom block\n",
    "model.transformer.h[layer_index] = GPT2BlockWithSteering(model.config, combined[layer_index],scalar = 5)\n",
    "\n",
    "tokenized = tokenizer(\"The cat (Felis catus), also referred\",return_tensors= \"pt\")\n",
    "out = model.generate(tokenized.input_ids, pad_token_id=tokenizer.eos_token_id,max_new_tokens = 40)\n",
    "print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marki/Documents/GitHub/KDS_MI/utils/create_bible_data.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  complement[int(layer)] = torch.load(str(steering_vector_path + vector),map_location=torch.device(device))\n",
      "/Users/marki/Documents/GitHub/KDS_MI/utils/create_bible_data.py:76: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target[int(layer)] = torch.load(str(steering_vector_path +vector),map_location=torch.device(device))\n",
      "/Users/marki/Documents/GitHub/KDS_MI/utils/create_bible_data.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined[int(layer)] = torch.load(str(steering_vector_path +vector),map_location=torch.device(device))\n"
     ]
    }
   ],
   "source": [
    "from utils.probe_confidence_intervals import model_setup\n",
    "from utils.create_bible_data import load_targeted_steering_vectors\n",
    "from classes.hook_manager import HookManager\n",
    "\n",
    "\n",
    "model, tokenizer, device = model_setup(\"AI-Sweden-Models/gpt-sw3-356m\")\n",
    "target, complement, combined = load_targeted_steering_vectors(\"steering_vectors/DA/\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus is a little for ungodelig, men det er jo ikke noget, der er sÃ¥ meget ved det.\n",
      "\n",
      "Det er jo ikke noget, der er sÃ¥ meget ved det, men det er jo ikke noget\n"
     ]
    }
   ],
   "source": [
    "layer = 13\n",
    "steering_lambda = 15\n",
    "steering_vector = combined[layer]\n",
    "\n",
    "with HookManager(model) as hook_manager:\n",
    "\n",
    "    hook_manager.attach_residual_stream_activation_based_steering_vector(\n",
    "        layer=layer,\n",
    "        steering_vector=steering_vector.to(device),\n",
    "        plus=True,\n",
    "        scalar=steering_lambda,\n",
    "        pre_mlp=False,\n",
    "        pythia=False\n",
    "    )\n",
    "    tokenized = tokenizer(\"Jesus is a little\",return_tensors= \"pt\")\n",
    "    out = model.generate(tokenized.input_ids, pad_token_id=tokenizer.eos_token_id,max_new_tokens = 40)\n",
    "    print(tokenizer.decode(out[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1049,  0.4062, -0.0869,  ...,  0.2007, -0.3405, -0.0782])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "from torch import Tensor\n",
    "class SteeringConfig(PretrainedConfig):\n",
    "    model_type = \"steering-gptsw3\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        #steering_vector= None,\n",
    "        layer:   int   = 13,\n",
    "        plus:    bool  = True,\n",
    "        scalar:  float = 1.0,\n",
    "        pre_mlp: bool  = False,\n",
    "        pythia:  bool  = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \n",
    "        self.layer   = layer\n",
    "        self.plus    = plus\n",
    "        self.scalar  = scalar\n",
    "        self.pre_mlp = pre_mlp\n",
    "        self.pythia  = pythia\n",
    "        self.steering_vector = steering_vector\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SteeringConfig_config = SteeringConfig( layer = 13, plus= True, scalar = 1000, pre_mlp = False, pythia = False)\n",
    "SteeringConfig_config.save_pretrained(\"steering-gptsw3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class AwesomeGPTsw3(PreTrainedModel):\n",
    "    config_class = SteeringConfig\n",
    "    def __init__(self, config_class: SteeringConfig, pre_trained_model, steering_vector):\n",
    "        super().__init__(config_class)\n",
    "        self.model = pre_trained_model\n",
    "        self.steering_vector = steering_vector\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        with HookManager(model) as hook_manager:\n",
    "            hook_manager.attach_residual_stream_activation_based_steering_vector(\n",
    "                layer=self.config_class.layer,\n",
    "                steering_vector=self.steering_vector,\n",
    "                plus=True,\n",
    "                scalar=steering_lambda,\n",
    "                pre_mlp=False,\n",
    "                pythia=False\n",
    "            )\n",
    "            idk = self.model.forward(**kwargs)\n",
    "        return idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoModelForCausalLM\n",
    "\n",
    "class SteeringGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    config_class = SteeringConfig\n",
    "    base_model_prefix = \"transformer\"  # same prefix GPT2LMHeadModel uses\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        pretrained_model_name_or_path,\n",
    "        steering_vector: torch.Tensor = None,\n",
    "        layer:   int   = None,\n",
    "        plus:    bool  = None,\n",
    "        scalar:  float = None,\n",
    "        pre_mlp: bool  = None,\n",
    "        pythia:  bool  = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # 1) Load config (merge any overrides)\n",
    "        config = SteeringConfig.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            layer=layer,\n",
    "            plus=plus,\n",
    "            scalar=scalar,\n",
    "            pre_mlp=pre_mlp,\n",
    "            pythia=pythia,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # 2) Load GPT-2 weights into our subclass\n",
    "        model: SteeringGPT2LMHeadModel = super().from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            config=config,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # 3) Attach your HookManager if a vector is provided\n",
    "        if steering_vector is not None:\n",
    "            model.hook_manager = HookManager(model)\n",
    "            model.hook_manager.attach_residual_stream_activation_based_steering_vector(\n",
    "                layer=config.layer,\n",
    "                steering_vector=steering_vector.to(model.device),\n",
    "                plus=config.plus,\n",
    "                scalar=config.scalar,\n",
    "                pre_mlp=config.pre_mlp,\n",
    "                pythia=config.pythia,\n",
    "            )\n",
    "        return model\n",
    "\n",
    "    # (no need to override forward; inherited from GPT2LMHeadModel)\n",
    "# so AutoModelForCausalLM.from_pretrained picks up your class\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "\n",
    "AutoConfig.register(\"steering-gpt2\", SteeringConfig)\n",
    "AutoModelForCausalLM.register(SteeringConfig, SteeringGPT2LMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"my-steering-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GPT2LMHeadModel.__init__() got an unexpected keyword argument 'steering_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI-Sweden-Models/gpt-sw3-356m\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-steering-model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteering_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# your torch.Tensor\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# (optionally) override layer/plus/scalar/etc here\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:4342\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4336\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   4337\u001b[0m         config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   4338\u001b[0m     )\n\u001b[1;32m   4340\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[1;32m   4341\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 4342\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4344\u001b[0m \u001b[38;5;66;03m# Make sure to tie the weights correctly\u001b[39;00m\n\u001b[1;32m   4345\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "\u001b[0;31mTypeError\u001b[0m: GPT2LMHeadModel.__init__() got an unexpected keyword argument 'steering_vector'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"AI-Sweden-Models/gpt-sw3-356m\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"my-steering-model\",\n",
    "    steering_vector=combined[15],       # your torch.Tensor\n",
    "    # (optionally) override layer/plus/scalar/etc here\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
