{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "found device: cpu\n",
      "Load data\n",
      "Extract activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/130 [01:02<44:16, 20.92s/it]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from refactor.utils.data import FilePaths, load_antibiotic_data\n",
    "from refactor.utils.hooking import get_activations as get_activations_new\n",
    "from refactor.utils.compatibility import ModelConfig\n",
    "from refactor.probes import model_setup\n",
    "from utils.probe_confidence_intervals import bootstrap\n",
    "\n",
    "\"\"\"This function runs an entire pipeline that bootstraps, trains and creates confidence intervals showing\n",
    "    The probes f1 score on different labels and across layers\n",
    "    \n",
    "    We bootstrap 10 times\n",
    "    Results are saved in this folder: results/data/probe_confidence_intervals/*model_name*_reg_lambda_*reg_lambda*\n",
    "\n",
    "Args:\n",
    "    model_name (_type_): _description_\n",
    "    reg_lambdas (_type_): _description_\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"downloaded_models/gpt_gptsw3_en_is_da_356m_gbs1024\"\n",
    "\n",
    "\n",
    "# loads model\n",
    "print(\"Load model\")\n",
    "model, tokenizer, device = model_setup(model_name)\n",
    "\n",
    "\n",
    "# loads data\n",
    "print(\"Load data\")\n",
    "ds = load_antibiotic_data(\n",
    "    file_paths=FilePaths.antibiotic,\n",
    "    file_extension='txt'\n",
    ")\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# sets training parameters\n",
    "meta_data = {}\n",
    "meta_data[\"hidden_size\"] = ModelConfig.hidden_size(model)\n",
    "meta_data[\"hidden_layers\"] = ModelConfig.hidden_layers(model)\n",
    "meta_data[\"model_name\"] = model_name.split(\"/\")[0]\n",
    "meta_data[\"learning_rate\"] = 0.001\n",
    "meta_data[\"reg_lambda\"] = 10\n",
    "meta_data[\"amount_epochs\"] = 1\n",
    "\n",
    "\n",
    "# extracts activation from forward passes on data\n",
    "# We use hooks to extract the different layer activations that will be used to train our probes\n",
    "\n",
    "print(\"Extract activations\")\n",
    "activations = get_activations_new(\n",
    "    loader=loader, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    hook_addresses=None,\n",
    "    layers=None,\n",
    "    max_batches=2,\n",
    "    sampling_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = []\n",
    "for index, (key, val) in enumerate(activations.items()):\n",
    "    if index == 6: break\n",
    "    positions.append(key.replace(\"layer.0.\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for pos in positions:\n",
    "    acts_ds_by_layer = {}\n",
    "    for layer in range(meta_data[\"hidden_layers\"]):\n",
    "        pos_key = f\"layer.{layer}.{pos}\"\n",
    "        acts_ds_by_layer[layer] = activations[pos_key]\n",
    "    d[pos] = acts_ds_by_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 6/130 [00:01<00:37,  3.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# extracts activation from forward passes on data\n",
    "# We use hooks to extract the different layer activations that will be used to train our probes\n",
    "from utils.probe_confidence_intervals import get_activations\n",
    "\n",
    "print(\"Extract activations\")\n",
    "#activation_ds_by_layer = get_activations(meta_data,loader, tokenizer, device, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "for i in range(meta_data[\"hidden_layers\"]):\n",
    "    unique_labels = set(np.array(acts_ds_by_layer[i].labels))\n",
    "    [s.add(x) for x in unique_labels]\n",
    "number_labels = len(s)\n",
    "meta_data[\"number_labels\"] = number_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot = bootstrap(10, meta_data, acts_ds_by_layer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
