{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'da': ('I Begyndelsen skabte Gud Himmelen og Jorden.',\n",
       "  'Og Jorden var øde og tom, og der var Mørke over Verdensdybet. Men Guds Ånd svævede over Vandene.'),\n",
       " 'en': ('In the beginning God created the heavens and the earth.',\n",
       "  \"Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters.\")}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dansk bibel                            \"bla bla (originalt danski)\" \n",
    "#engelsk bibel \"bla bla\" steering homie \"bla bla (men dansk)\"\n",
    "from huggingface_hub import hf_hub_download\n",
    "from utils.steering import generate_with_steering\n",
    "from transformers import AutoModelForCausalLM\n",
    "import fasttext\n",
    "import torch\n",
    "import os\n",
    "from classes.datahandling import ParallelNSPDataset\n",
    "from utils.probe_confidence_intervals import model_setup\n",
    "model, tokenizer, device = model_setup(\"AI-Sweden-Models/gpt-sw3-356m\")\n",
    "\n",
    "model_name = \"nb-nordic-lid.ftz\"\n",
    "language_prediction_model = fasttext.load_model(hf_hub_download(\"NbAiLab/nb-nordic-lid\", model_name))\n",
    "\n",
    "label, score = language_prediction_model.predict(\"Harry Potter är en serie fantasyromaner av författaren J.K. Rowling, som började ges\", threshold=0.25)\n",
    "label[0].split(\"__\")[-1], score\n",
    "bible_data = ParallelNSPDataset.from_tmx(\"data/bible-da-en.tmx\",\"da\",\"en\")\n",
    "bible_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/267bnxr16cq2xpr1crtpxbgw0000gn/T/ipykernel_2316/835105919.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  complement[int(layer)] = torch.load(str(steering_vector_path + vector))\n",
      "/var/folders/p0/267bnxr16cq2xpr1crtpxbgw0000gn/T/ipykernel_2316/835105919.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  target[int(layer)] = torch.load(str(steering_vector_path +vector))\n",
      "/var/folders/p0/267bnxr16cq2xpr1crtpxbgw0000gn/T/ipykernel_2316/835105919.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  combined[int(layer)] = torch.load(str(steering_vector_path +vector))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_targeted_steering_vectors(steering_vector_path: str) -> tuple[dict,dict,dict]:\n",
    "    \"\"\"loads steering vectors that are targeted towards a language. \n",
    "    it returns the target, complement and combined, with combined = target - complement\n",
    "\n",
    "    Args:\n",
    "        steering_vector_path (str): some path\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict,dict,dict]: target, complement, combined\n",
    "    \"\"\"\n",
    "    combined = dict()\n",
    "    complement = dict()\n",
    "    target = dict()\n",
    "    for vector in os.listdir(steering_vector_path):\n",
    "        type = vector.split(\"_\")[0]\n",
    "        layer = vector.split(\"_\")[4]\n",
    "        if type == \"combined\":\n",
    "            combined[int(layer)] = torch.load(str(steering_vector_path +vector))\n",
    "        elif type == \"complement\":\n",
    "            complement[int(layer)] = torch.load(str(steering_vector_path + vector))\n",
    "        elif type == \"target\":\n",
    "            target[int(layer)] = torch.load(str(steering_vector_path +vector))\n",
    "    return target, complement, combined\n",
    "\n",
    "steering_vector_path = \"steering_vectors/test_run_2/\"\n",
    "\n",
    "target, complement, combined = load_targeted_steering_vectors(steering_vector_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0917,  0.2917, -0.0411,  ...,  0.1085, -0.4137, -0.0248])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserts a steering vector and shifts the model towards that direction. \n",
    "def gen_outputs(bible_data:ParallelNSPDataset, \n",
    "                language_1:str,\n",
    "                language_2:str, \n",
    "                bible_index:int, \n",
    "                layer:int,\n",
    "                steering_vector:torch.Tensor,\n",
    "                steering_lambda:int,\n",
    "                model:AutoModelForCausalLM) -> tuple:\n",
    "    \"\"\"Inserts a steering vector and shifts the model towards that direction. \n",
    "    If we want to shift a model from example english to danish, then we set language_1 = \"da\" and language_2 = \"en\"\n",
    "    Additionally the steering vector should be the one steering towards danish.\n",
    "\n",
    "    Args:\n",
    "        bible_data (ParallelNSPDataset): dataset with bible data\n",
    "        language_1 (str): the language you want to steer towards\n",
    "        language_2 (str): the language you steer away from\n",
    "        bible_index (int): index of a given verse in the bible\n",
    "        layer (int): layer of the model where you want to insert the steering vector\n",
    "        steering_vector (torch.Tensor): the steering vector\n",
    "        steering_lambda (int): the strenght of the steering vector\n",
    "        model (AutoModelForCausalLM): the model you want to use\n",
    "\n",
    "    Returns:\n",
    "        tuple: _description_\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    language_1_prompt = bible_data[bible_index][language_1][0].lower()\n",
    "    language_1_true_bible_verse = bible_data[bible_index][language_1][1]\n",
    "    \n",
    "    language_2_prompt = bible_data[bible_index][language_2][0].lower()\n",
    "    language_2_true_bible_verse = bible_data[bible_index][language_2][1]\n",
    "    \n",
    "    input_ids = tokenizer(language_1_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=30, do_sample=True)[0]\n",
    "    language_1_predicted_bible_verse = tokenizer.decode(generated_token_ids)[len(language_1_prompt):]\n",
    "    \n",
    "    language_2_predicted_bible_verse = generate_with_steering(model,tokenizer,layer,language_2_prompt,steering_vector[layer], steering_lambda= steering_lambda)\n",
    "    language_2_predicted_bible_verse = language_2_predicted_bible_verse[0][len(language_2_prompt):]\n",
    "    \n",
    "    return language_1_predicted_bible_verse, language_2_predicted_bible_verse, language_1_true_bible_verse,language_2_true_bible_verse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "danish_predicted_output_list = []\n",
    "english_predicted_output_list = []\n",
    "danish_true_label_list = []\n",
    "english_true_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 15\n",
    "danish_predicted_output, english_predicted_output, danish_true_label,english_true_label = gen_outputs(bible_data, \"da\",\"en\",50,layer,combined,5, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Så lod Gud HERREN Dvale falde over Adam, og da han var sovet ind, tog han et af hans Ribben og lukkede med Kød i dets Sted;'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danish_true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I denne synd har hans tjener Adams søn syndet. Jesus Kristus har et stort ansvar for de mennesker, der kommer i hans efterfølgelse, for'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "danish_predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yahweh God caused a deep sleep to fall on the man, and he slept; and he took one of his ribs, and closed up the flesh in its place.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_true_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  6:12-13: \"Men skal ikke blive til jord, men til himmel, men til støv, og til sten og til støv; men til himmel skal de blive, og til jord skal de blive til støv, og til sten skal de blive til støv, og til støv skal de'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_predicted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
