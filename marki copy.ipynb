{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "found device: cpu\n",
      "Load data\n",
      "Extract activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  2%|▏         | 3/130 [00:01<01:19,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from refactor.utils.data import FilePaths, load_antibiotic_data\n",
    "from refactor.utils.hooking import get_activations as get_activations_new\n",
    "from refactor.utils.compatibility import ModelConfig\n",
    "from refactor.probes import model_setup\n",
    "from utils.probe_confidence_intervals import bootstrap\n",
    "\n",
    "\"\"\"This function runs an entire pipeline that bootstraps, trains and creates confidence intervals showing\n",
    "    The probes f1 score on different labels and across layers\n",
    "    \n",
    "    We bootstrap 10 times\n",
    "    Results are saved in this folder: results/data/probe_confidence_intervals/*model_name*_reg_lambda_*reg_lambda*\n",
    "\n",
    "Args:\n",
    "    model_name (_type_): _description_\n",
    "    reg_lambdas (_type_): _description_\n",
    "\"\"\"\n",
    "\n",
    "model_name = \"downloaded_models/gpt_gptsw3_en_is_da_356m_gbs1024\"\n",
    "model_name = \"EleutherAI/pythia-14m\"\n",
    "\n",
    "# loads model\n",
    "print(\"Load model\")\n",
    "model, tokenizer, device = model_setup(model_name)\n",
    "\n",
    "\n",
    "# loads data\n",
    "print(\"Load data\")\n",
    "ds = load_antibiotic_data(\n",
    "    file_paths=FilePaths.antibiotic,\n",
    "    file_extension='txt'\n",
    ")\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# sets training parameters\n",
    "meta_data = {}\n",
    "meta_data[\"hidden_size\"] = ModelConfig.hidden_size(model)\n",
    "meta_data[\"hidden_layers\"] = ModelConfig.hidden_layers(model)\n",
    "meta_data[\"model_name\"] = model_name.split(\"/\")[0]\n",
    "meta_data[\"learning_rate\"] = 0.001\n",
    "meta_data[\"reg_lambda\"] = 10\n",
    "meta_data[\"amount_epochs\"] = 1\n",
    "\n",
    "\n",
    "# extracts activation from forward passes on data\n",
    "# We use hooks to extract the different layer activations that will be used to train our probes\n",
    "\n",
    "print(\"Extract activations\")\n",
    "activations = get_activations_new(\n",
    "    loader=loader, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    hook_addresses=None,\n",
    "    layers=None,\n",
    "    max_batches=2,\n",
    "    sampling_prob=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = []\n",
    "for index, (key, val) in enumerate(activations.items()):\n",
    "    if index == 6: break\n",
    "    positions.append(key.replace(\"layer.0.\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for pos in positions:\n",
    "    acts_ds_by_layer = {}\n",
    "    for layer in range(meta_data[\"hidden_layers\"]):\n",
    "        pos_key = f\"layer.{layer}.{pos}\"\n",
    "        acts_ds_by_layer[layer] = activations[pos_key]\n",
    "    d[pos] = acts_ds_by_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <refactor.utils.data.ActivationDataset at 0x30f6a9d30>,\n",
       " 1: <refactor.utils.data.ActivationDataset at 0x310366f00>,\n",
       " 2: <refactor.utils.data.ActivationDataset at 0x3103d46b0>,\n",
       " 3: <refactor.utils.data.ActivationDataset at 0x3103d47d0>,\n",
       " 4: <refactor.utils.data.ActivationDataset at 0x3103d49e0>,\n",
       " 5: <refactor.utils.data.ActivationDataset at 0x3103d4c20>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acts_ds_by_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract activations\n"
     ]
    }
   ],
   "source": [
    "# extracts activation from forward passes on data\n",
    "# We use hooks to extract the different layer activations that will be used to train our probes\n",
    "from utils.probe_confidence_intervals import get_activations\n",
    "\n",
    "print(\"Extract activations\")\n",
    "#activation_ds_by_layer = get_activations(meta_data,loader, tokenizer, device, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layernorm_1:pre': {0: <refactor.utils.data.ActivationDataset at 0x30f5f58b0>,\n",
       "  1: <refactor.utils.data.ActivationDataset at 0x30f0857c0>,\n",
       "  2: <refactor.utils.data.ActivationDataset at 0x310367410>,\n",
       "  3: <refactor.utils.data.ActivationDataset at 0x3103d46e0>,\n",
       "  4: <refactor.utils.data.ActivationDataset at 0x3103d4800>,\n",
       "  5: <refactor.utils.data.ActivationDataset at 0x3103d4a40>},\n",
       " 'attention:pre': {0: <refactor.utils.data.ActivationDataset at 0x30f65dfd0>,\n",
       "  1: <refactor.utils.data.ActivationDataset at 0x30f085550>,\n",
       "  2: <refactor.utils.data.ActivationDataset at 0x310366c90>,\n",
       "  3: <refactor.utils.data.ActivationDataset at 0x3103d4710>,\n",
       "  4: <refactor.utils.data.ActivationDataset at 0x3103d4860>,\n",
       "  5: <refactor.utils.data.ActivationDataset at 0x3103d4aa0>},\n",
       " 'attention:post': {0: <refactor.utils.data.ActivationDataset at 0x30f0d6120>,\n",
       "  1: <refactor.utils.data.ActivationDataset at 0x310366210>,\n",
       "  2: <refactor.utils.data.ActivationDataset at 0x3103d4620>,\n",
       "  3: <refactor.utils.data.ActivationDataset at 0x3103d4740>,\n",
       "  4: <refactor.utils.data.ActivationDataset at 0x3103d48c0>,\n",
       "  5: <refactor.utils.data.ActivationDataset at 0x3103d4b00>},\n",
       " 'layernorm_2:pre': {0: <refactor.utils.data.ActivationDataset at 0x31039fd40>,\n",
       "  1: <refactor.utils.data.ActivationDataset at 0x310366e40>,\n",
       "  2: <refactor.utils.data.ActivationDataset at 0x3103d4650>,\n",
       "  3: <refactor.utils.data.ActivationDataset at 0x3103d4770>,\n",
       "  4: <refactor.utils.data.ActivationDataset at 0x3103d4920>,\n",
       "  5: <refactor.utils.data.ActivationDataset at 0x3103d4b60>},\n",
       " 'mlp:pre': {0: <refactor.utils.data.ActivationDataset at 0x31039cd10>,\n",
       "  1: <refactor.utils.data.ActivationDataset at 0x310367230>,\n",
       "  2: <refactor.utils.data.ActivationDataset at 0x3103d4680>,\n",
       "  3: <refactor.utils.data.ActivationDataset at 0x3103d47a0>,\n",
       "  4: <refactor.utils.data.ActivationDataset at 0x3103d4980>,\n",
       "  5: <refactor.utils.data.ActivationDataset at 0x3103d4bc0>},\n",
       " 'mlp:post': {0: <refactor.utils.data.ActivationDataset at 0x30f6a9d30>,\n",
       "  1: <refactor.utils.data.ActivationDataset at 0x310366f00>,\n",
       "  2: <refactor.utils.data.ActivationDataset at 0x3103d46b0>,\n",
       "  3: <refactor.utils.data.ActivationDataset at 0x3103d47d0>,\n",
       "  4: <refactor.utils.data.ActivationDataset at 0x3103d49e0>,\n",
       "  5: <refactor.utils.data.ActivationDataset at 0x3103d4c20>}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <refactor.utils.data.ActivationDataset at 0x30f65dfd0>,\n",
       " 1: <refactor.utils.data.ActivationDataset at 0x30f085550>,\n",
       " 2: <refactor.utils.data.ActivationDataset at 0x310366c90>,\n",
       " 3: <refactor.utils.data.ActivationDataset at 0x3103d4710>,\n",
       " 4: <refactor.utils.data.ActivationDataset at 0x3103d4860>,\n",
       " 5: <refactor.utils.data.ActivationDataset at 0x3103d4aa0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"attention:pre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = set()\n",
    "for i in range(meta_data[\"hidden_layers\"]):\n",
    "    unique_labels = set(np.array(acts_ds_by_layer[i].labels))\n",
    "    [s.add(x) for x in unique_labels]\n",
    "number_labels = len(s)\n",
    "meta_data[\"number_labels\"] = number_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot = bootstrap(10, meta_data, acts_ds_by_layer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layernorm_1:pre\n",
      "attention:pre\n",
      "attention:post\n",
      "layernorm_2:pre\n",
      "mlp:pre\n",
      "mlp:post\n"
     ]
    }
   ],
   "source": [
    "for pos in positions:\n",
    "    print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "found device: cpu\n",
      "Load data\n",
      "Extract activations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/130 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "  2%|▏         | 3/130 [00:01<00:58,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from refactor.utils.data import FilePaths, load_antibiotic_data\n",
    "from refactor.utils.hooking import get_activations as get_activations_new\n",
    "from refactor.utils.compatibility import ModelConfig\n",
    "from refactor.probes import model_setup\n",
    "from utils.probe_confidence_intervals import bootstrap\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\"\"\"This function runs an entire pipeline that bootstraps, trains and creates confidence intervals showing\n",
    "    The probes f1 score on different labels and across layers\n",
    "    \n",
    "    We bootstrap 10 times\n",
    "    Results are saved in this folder: results/data/probe_confidence_intervals/*model_name*_reg_lambda_*reg_lambda*\n",
    "\n",
    "Args:\n",
    "    model_name (_type_): _description_\n",
    "    reg_lambdas (_type_): _description_\n",
    "\"\"\"\n",
    "\n",
    "# loads model\n",
    "print(\"Load model\")\n",
    "model, tokenizer, device = model_setup(model_name)\n",
    "\n",
    "\n",
    "# loads data\n",
    "print(\"Load data\")\n",
    "ds = load_antibiotic_data(\n",
    "    file_paths=FilePaths.antibiotic,\n",
    "    file_extension='txt'\n",
    ")\n",
    "loader = DataLoader(ds, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# sets training parameters\n",
    "meta_data = {}\n",
    "meta_data[\"hidden_size\"] = ModelConfig.hidden_size(model)\n",
    "meta_data[\"hidden_layers\"] = ModelConfig.hidden_layers(model)\n",
    "meta_data[\"model_name\"] = model_name.split(\"/\")[0]\n",
    "meta_data[\"learning_rate\"] = 0.001\n",
    "meta_data[\"reg_lambda\"] = 10\n",
    "meta_data[\"amount_epochs\"] = 1\n",
    "\n",
    "\n",
    "# extracts activation from forward passes on data\n",
    "# We use hooks to extract the different layer activations that will be used to train our probes\n",
    "\n",
    "print(\"Extract activations\")\n",
    "activations = get_activations_new(\n",
    "    loader=loader, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    hook_addresses=None,\n",
    "    layers=None,\n",
    "    max_batches=2,\n",
    "    sampling_prob=0.1\n",
    ")\n",
    "\n",
    "#Here we get the different positions that we are testing\n",
    "positions = []\n",
    "for index, (key, val) in enumerate(activations.items()):\n",
    "    if index == 6: break\n",
    "    positions.append(key.replace(\"layer.0.\",\"\"))\n",
    "\n",
    "\n",
    "d = {}\n",
    "for pos in positions:\n",
    "    acts_ds_by_layer = {}\n",
    "    for layer in range(meta_data[\"hidden_layers\"]):\n",
    "        pos_key = f\"layer.{layer}.{pos}\"\n",
    "        acts_ds_by_layer[layer] = activations[pos_key]\n",
    "    d[pos] = acts_ds_by_layer\n",
    "\n",
    "#We extract the amount of labels. \n",
    "#We just do this for a single position as all of them shares the same labels\n",
    "s = set()\n",
    "for i in range(meta_data[\"hidden_layers\"]):\n",
    "    unique_labels = set(np.array(acts_ds_by_layer[i].labels))\n",
    "    [s.add(x) for x in unique_labels]\n",
    "number_labels = len(s)\n",
    "meta_data[\"number_labels\"] = number_labels\n",
    "\n",
    "\n",
    "\n",
    "data_output_folder = Path('results/data/probe_confidence_intervals')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: <refactor.utils.data.ActivationDataset object at 0x31039e360>, 1: <refactor.utils.data.ActivationDataset object at 0x30f6a9220>, 2: <refactor.utils.data.ActivationDataset object at 0x111cf73e0>, 3: <refactor.utils.data.ActivationDataset object at 0x31aba7170>, 4: <refactor.utils.data.ActivationDataset object at 0x316cf85f0>, 5: <refactor.utils.data.ActivationDataset object at 0x316cf8d10>} layernorm_1:pre hi\n",
      "[] attention:pre hi\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reg_lambda \u001b[38;5;129;01min\u001b[39;00m reg_lambdas:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#print()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     meta_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m reg_lambda\n\u001b[0;32m---> 10\u001b[0m     boot \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macts_ds_by_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     map_lab \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap_label\n\u001b[1;32m     14\u001b[0m     d \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/KDS_MI/utils/probe_confidence_intervals.py:226\u001b[0m, in \u001b[0;36mbootstrap\u001b[0;34m(n, meta_data, activation_ds_by_layer, device)\u001b[0m\n\u001b[1;32m    223\u001b[0m li \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[1;32m    225\u001b[0m    \u001b[38;5;66;03m#print(activation_ds_by_layer,\"grr\")\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m     new_activation_ds_by_layer \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bootstrap_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation_ds_by_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     probe_by_layer, act_loader_by_layer \u001b[38;5;241m=\u001b[39m create_classes_by_layer(meta_data, new_activation_ds_by_layer, device)\n\u001b[1;32m    228\u001b[0m     train_probe(meta_data, probe_by_layer,act_loader_by_layer, device)\n",
      "File \u001b[0;32m~/Documents/GitHub/KDS_MI/utils/probe_confidence_intervals.py:208\u001b[0m, in \u001b[0;36mcreate_bootstrap_dataset\u001b[0;34m(activation_ds_by_layer)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_bootstrap_dataset\u001b[39m(activation_ds_by_layer):\n\u001b[1;32m    206\u001b[0m     copy_dataset \u001b[38;5;241m=\u001b[39m deepcopy(activation_ds_by_layer)\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcopy_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m():\n\u001b[1;32m    209\u001b[0m         activations \u001b[38;5;241m=\u001b[39m copy_dataset[layer]\u001b[38;5;241m.\u001b[39mpredictors\n\u001b[1;32m    210\u001b[0m         labels \u001b[38;5;241m=\u001b[39m copy_dataset[layer]\u001b[38;5;241m.\u001b[39mlabels\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "model_name_temp = \"test\"\n",
    "reg_lambdas = [1,2]\n",
    "for pos in positions:\n",
    "    \n",
    "    acts_ds_by_layer = d[pos]\n",
    "    print(acts_ds_by_layer, pos, \"hi\")\n",
    "    for reg_lambda in reg_lambdas:\n",
    "        #print()\n",
    "        meta_data['reg_lambda'] = reg_lambda\n",
    "        boot = bootstrap(10, meta_data, acts_ds_by_layer, device)\n",
    "        map_lab = ds.map_label\n",
    "\n",
    "        \n",
    "        d = defaultdict(list)\n",
    "        for run in boot:\n",
    "            for layer in run.keys():\n",
    "                class_accuracies = run[layer].class_accuracies\n",
    "                d[layer].append(class_accuracies)\n",
    "\n",
    "\n",
    "\n",
    "        # saves data used in plots\n",
    "        \n",
    "        d['map_label'] = map_lab\n",
    "\n",
    "        reg_lambda_output_file = data_output_folder / f\"{model_name_temp}_{pos}_reg_lambda_{meta_data['reg_lambda']}.json\"\n",
    "        with open(str(reg_lambda_output_file), 'w') as file:\n",
    "            json.dump(d, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
