
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.35518607021558046, "macro_f1": 0.43526041053416537}, {"mcc": 0.27918473011692274, "macro_f1": 0.37762957123088375}, {"mcc": 0.37229822078705205, "macro_f1": 0.5633428015916495}, {"mcc": 0.3366718281433893, "macro_f1": 0.40391143537518365}, {"mcc": 0.37457968221213434, "macro_f1": 0.44969135802469135}, {"mcc": 0.2097969770636337, "macro_f1": 0.355657962109575}, {"mcc": 0.30410057061104384, "macro_f1": 0.42119992342631196}, {"mcc": 0.3946382483636516, "macro_f1": 0.452192378328742}, {"mcc": 0.3457724202803098, "macro_f1": 0.43257534292017047}, {"mcc": 0.4318769956363507, "macro_f1": 0.4599063636905827}]}, "total": {"test_mcc": 34.04105743430069, "test_mcc_se": 3.9134105953203524, "test_macro_f1": 43.513675472319555, "test_macro_f1_se": 3.478829229847264}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.23856209150326796, "micro_f1": 0.21529745042492918}, {"micro_f1_no_misc": 0.14942528735632185, "micro_f1": 0.13666666666666666}, {"micro_f1_no_misc": 0.006802721088435374, "micro_f1": 0.03985507246376812}, {"micro_f1_no_misc": 0.1900647948164147, "micro_f1": 0.16544117647058823}, {"micro_f1_no_misc": 0.1254125412541254, "micro_f1": 0.12012012012012012}, {"micro_f1_no_misc": 0.14545454545454548, "micro_f1": 0.12751677852348994}, {"micro_f1_no_misc": 0.15827338129496404, "micro_f1": 0.1437699680511182}, {"micro_f1_no_misc": 0.11960132890365448, "micro_f1": 0.11680911680911683}, {"micro_f1_no_misc": 0.1757105943152455, "micro_f1": 0.1320450885668277}, {"micro_f1_no_misc": 0.12802768166089964, "micro_f1": 0.1394658753709199}]}, "total": {"test_micro_f1_no_misc": 14.373349676478744, "test_micro_f1_no_misc_se": 3.714421618176756, "test_micro_f1": 13.36987313467545, "test_micro_f1_se": 2.7037782498405227}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.030455672411361313, "macro_f1": 0.3801227083219546}, {"mcc": 0.0, "macro_f1": 0.46440013076168685}, {"mcc": -0.020828752216792853, "macro_f1": 0.36955724390882894}, {"mcc": -0.003944481036418488, "macro_f1": 0.4487823605014104}, {"mcc": 0.08561667029818171, "macro_f1": 0.3729241807626285}, {"mcc": -0.0872797785719991, "macro_f1": 0.34091396695987985}, {"mcc": -0.02430938013719691, "macro_f1": 0.419441744675772}, {"mcc": 0.051639777949432225, "macro_f1": 0.36318407960199}, {"mcc": 0.10685243369373369, "macro_f1": 0.41013824884792627}, {"mcc": -0.05588366139390376, "macro_f1": 0.3758722152164775}]}, "total": {"test_mcc": 0.8231850099639784, "test_mcc_se": 3.778352861821553, "test_macro_f1": 39.453368795585554, "test_macro_f1_se": 2.4613196189979685}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"em": 38.69047619047619, "f1": 42.465913715913715}, {"em": 40.0, "f1": 43.38385225885226}, {"em": 34.93975903614458, "f1": 38.97499087258123}, {"em": 36.30952380952381, "f1": 40.50075585789872}, {"em": 40.645161290322584, "f1": 45.5657962109575}, {"em": 37.64705882352941, "f1": 42.58349493643611}, {"em": 34.355828220858896, "f1": 37.88684389911384}, {"em": 26.70807453416149, "f1": 30.871461182020184}, {"em": 33.54430379746835, "f1": 38.600562587904356}, {"em": 35.15151515151515, "f1": 40.20446893174166}]}, "total": {"test_em": 35.799170085400036, "test_em_se": 2.480423066762068, "test_f1": 40.103814045341956, "test_f1_se": 2.497863701454097}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.14136913641191975, "macro_f1": 0.3114638737471493}, {"mcc": 0.0, "macro_f1": 0.14634146341463414}, {"mcc": 0.03534319192453339, "macro_f1": 0.22649616022192906}, {"mcc": 0.08559226427019176, "macro_f1": 0.21515953367080842}, {"mcc": 0.10359896155113996, "macro_f1": 0.2400141392718275}, {"mcc": 0.12125734759442537, "macro_f1": 0.3411540214171793}, {"mcc": 0.31160394248745316, "macro_f1": 0.3956073708341205}, {"mcc": 0.2536962249684162, "macro_f1": 0.4224724372507624}, {"mcc": 0.09017524047903357, "macro_f1": 0.22783389450056116}, {"mcc": 0.1729435045194054, "macro_f1": 0.41152443431925945}]}, "total": {"test_mcc": 13.155798142065183, "test_mcc_se": 5.858957513075483, "test_macro_f1": 29.380673286482317, "test_macro_f1_se": 5.953490466921334}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.02824858757062147, "micro_f1": 0.05614035087719299}, {"micro_f1_no_misc": 0.01038961038961039, "micro_f1": 0.01553398058252427}, {"micro_f1_no_misc": 0.08421052631578947, "micro_f1": 0.08805031446540881}, {"micro_f1_no_misc": 0.09130434782608694, "micro_f1": 0.08872458410351203}, {"micro_f1_no_misc": 0.12788632326820604, "micro_f1": 0.12520064205457465}, {"micro_f1_no_misc": 0.0915032679738562, "micro_f1": 0.13043478260869565}, {"micro_f1_no_misc": 0.07615230460921844, "micro_f1": 0.07704160246533127}, {"micro_f1_no_misc": 0.011080332409972301, "micro_f1": 0.1296551724137931}, {"micro_f1_no_misc": 0.053571428571428575, "micro_f1": 0.04245283018867924}, {"micro_f1_no_misc": 0.10993657505285413, "micro_f1": 0.11447811447811447}]}, "total": {"test_micro_f1_no_misc": 6.8428330398764405, "test_micro_f1_no_misc_se": 2.5400399213334115, "test_micro_f1": 8.677123742378265, "test_micro_f1_se": 2.4504149469700907}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"bertscore": 0.5768308055121452, "rouge_l": 0.11530059474170634}, {"bertscore": 0.5749611138598993, "rouge_l": 0.1167863073987071}, {"bertscore": 0.5708496569423005, "rouge_l": 0.09917787148676435}, {"bertscore": 0.628724463400431, "rouge_l": 0.14530517526538414}, {"bertscore": 0.5823411884484813, "rouge_l": 0.12025151260666536}, {"bertscore": 0.6220454624854028, "rouge_l": 0.14332318012335116}, {"bertscore": 0.5721241178689525, "rouge_l": 0.10384514701336606}, {"bertscore": 0.5979103631107137, "rouge_l": 0.12629304877231762}, {"bertscore": 0.597724124090746, "rouge_l": 0.13625657396434515}, {"bertscore": 0.6268335273489356, "rouge_l": 0.15114196714475564}]}, "total": {"test_bertscore": 59.50344823068008, "test_bertscore_se": 1.4457007860873563, "test_rouge_l": 12.576813785173629, "test_rouge_l_se": 1.1034034193295614}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.05115465928055023, "macro_f1": 0.44305590690614727}, {"mcc": -0.04706497621604712, "macro_f1": 0.3698017476275486}, {"mcc": 0.01800250112619768, "macro_f1": 0.36600922906978817}, {"mcc": 0.011380752593945711, "macro_f1": 0.48808867755192015}, {"mcc": 0.10321442750728609, "macro_f1": 0.4892328861369109}, {"mcc": 0.009665607189440706, "macro_f1": 0.48840927258193445}, {"mcc": -0.023417157276098777, "macro_f1": 0.47619047619047616}, {"mcc": -0.11514154661795957, "macro_f1": 0.4314314314314315}, {"mcc": -0.13026411630938972, "macro_f1": 0.393939393939394}, {"mcc": -0.016265001215808886, "macro_f1": 0.4898785425101214}]}, "total": {"test_mcc": -2.410441684989841, "test_mcc_se": 4.196677138723446, "test_macro_f1": 44.36037563945673, "test_macro_f1_se": 3.1573222611431686}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": -0.028384140682101006, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.3046875}, {"mcc": 0.10088634083907931, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.296875}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.328125}, {"mcc": 0.0747959184681732, "accuracy": 0.328125}, {"mcc": 0.024919897531243386, "accuracy": 0.265625}]}, "total": {"test_mcc": 1.722180161563949, "test_mcc_se": 2.464828426118755, "test_accuracy": 29.921874999999996, "test_accuracy_se": 1.7980871515346213}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.03046480879906189, "accuracy": 0.3359375}, {"mcc": 0.12131496817443596, "accuracy": 0.4140625}, {"mcc": 0.11469464011865305, "accuracy": 0.3671875}, {"mcc": 0.04726310723560305, "accuracy": 0.3828125}, {"mcc": -0.08445739633771641, "accuracy": 0.296875}, {"mcc": 0.07878340693893382, "accuracy": 0.3515625}, {"mcc": -0.0938954146352917, "accuracy": 0.359375}, {"mcc": 0.08705309281450903, "accuracy": 0.3828125}, {"mcc": 0.029052065519573258, "accuracy": 0.2890625}, {"mcc": 0.08230413460886296, "accuracy": 0.390625}]}, "total": {"test_mcc": 4.1257741323662485, "test_mcc_se": 4.682378313165761, "test_accuracy": 35.703125, "test_accuracy_se": 2.4906032168293195}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"em": 21.428571428571427, "f1": 25.255372616883008}, {"em": 26.25, "f1": 29.32854278074867}, {"em": 23.49397590361446, "f1": 26.606694156304357}, {"em": 25.595238095238095, "f1": 28.546974345293673}, {"em": 25.806451612903224, "f1": 29.744542196155102}, {"em": 8.823529411764707, "f1": 10.980392156862743}, {"em": 22.085889570552148, "f1": 24.696144306393315}, {"em": 8.074534161490684, "f1": 10.23070888434425}, {"em": 20.88607594936709, "f1": 22.825387155245682}, {"em": 23.03030303030303, "f1": 27.059687786960517}]}, "total": {"test_em": 20.547456916380487, "test_em_se": 4.115402081298903, "test_f1": 23.527444638519135, "test_f1_se": 4.424062602425179}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": 0.03827476197880848, "accuracy": 0.23828125}]}, "total": {"test_mcc": 0.3827476197880848, "test_mcc_se": 0.7501853347846462, "test_accuracy": 25.234374999999996, "test_accuracy_se": 1.0092836215947647}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"test_speed": 2189.46, "test_speed_short": 291.3}, {"test_speed": 3813.67, "test_speed_short": 405.65000000000003}, {"test_speed": 6571.75, "test_speed_short": 1128.8700000000001}, {"test_speed": 8154.99, "test_speed_short": 1388.28}, {"test_speed": 9714.539999999999, "test_speed_short": 1640.1}, {"test_speed": 11128.37, "test_speed_short": 2086.3399999999997}, {"test_speed": 10569.25, "test_speed_short": 1923.72}, {"test_speed": 13266.4, "test_speed_short": 2616.25}, {"test_speed": 15133.26, "test_speed_short": 2868.0}, {"test_speed": 15560.27, "test_speed_short": 3030.2000000000003}]}, "total": {"test_speed": 9610.196, "test_speed_se": 2791.2536696756856, "test_speed_short": 1737.8709999999999, "test_speed_short_se": 593.2346414490478}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.5760471230605617, "rouge_l": 0.09705440079024233}, {"bertscore": 0.5524321855045855, "rouge_l": 0.09902130289160324}, {"bertscore": 0.5577755308477208, "rouge_l": 0.07976092448376546}, {"bertscore": 0.4145923152100295, "rouge_l": 0.06110526104327817}, {"bertscore": 0.5287695411825553, "rouge_l": 0.08861783501955167}, {"bertscore": 0.5560010847402737, "rouge_l": 0.10270217779267202}, {"bertscore": 0.5504610406933352, "rouge_l": 0.08959344005072686}, {"bertscore": 0.5459430078044534, "rouge_l": 0.08978267818001737}, {"bertscore": 0.5001479594502598, "rouge_l": 0.09506851106101627}, {"bertscore": 0.48966777918394655, "rouge_l": 0.08889251091063122}]}, "total": {"test_bertscore": 52.71837567677722, "test_bertscore_se": 2.958300153307908, "test_rouge_l": 8.915990422235046, "test_rouge_l_se": 0.7316808404786563}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.02269037410444548, "accuracy": 0.23046875}, {"mcc": -0.04669612484854005, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": -0.0349143066109712, "accuracy": 0.25390625}, {"mcc": 0.04512502380174535, "accuracy": 0.21875}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": -0.005908121662595136, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": -0.04881773382197126, "accuracy": 0.25390625}]}, "total": {"test_mcc": -0.685208890378868, "test_mcc_se": 1.8362297475873246, "test_accuracy": 25.039062499999996, "test_accuracy_se": 2.4543824736220134}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.08719555876819683, "accuracy": 0.265625}, {"mcc": -0.146596312255935, "accuracy": 0.2421875}, {"mcc": 0.006079086967007631, "accuracy": 0.3125}, {"mcc": -0.07444894451550219, "accuracy": 0.3203125}, {"mcc": 0.025096996765459356, "accuracy": 0.171875}, {"mcc": -0.06068131301070984, "accuracy": 0.1875}, {"mcc": 0.06854232442041162, "accuracy": 0.3984375}, {"mcc": -0.09008481160447392, "accuracy": 0.28125}, {"mcc": 0.06925842979160136, "accuracy": 0.390625}, {"mcc": -0.06635270537589283, "accuracy": 0.296875}]}, "total": {"test_mcc": -3.5638280758623067, "test_mcc_se": 4.540412397323967, "test_accuracy": 28.671875000000004, "test_accuracy_se": 4.6503972584563495}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.06526396427506816, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": -0.056854363636301815, "accuracy": 0.21875}, {"mcc": 0.07290316944026093, "accuracy": 0.23046875}, {"mcc": -2.1497285501961673e-05, "accuracy": 0.25}, {"mcc": 0.017523278269778154, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.02776335937414326, "accuracy": 0.23828125}, {"mcc": 0.07803657660623552, "accuracy": 0.27734375}]}, "total": {"test_mcc": 2.046144870436822, "test_mcc_se": 2.5959247882281122, "test_accuracy": 25.5078125, "test_accuracy_se": 1.4348519730138733}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 1967.42, "test_speed_short": 233.70000000000002}, {"test_speed": 3757.5600000000004, "test_speed_short": 436.80999999999995}, {"test_speed": 5124.61, "test_speed_short": 840.64}, {"test_speed": 6570.2, "test_speed_short": 1048.34}, {"test_speed": 7558.760000000001, "test_speed_short": 1236.9499999999998}, {"test_speed": 8769.61, "test_speed_short": 1608.19}, {"test_speed": 9944.56, "test_speed_short": 1822.8600000000001}, {"test_speed": 10973.62, "test_speed_short": 1976.52}, {"test_speed": 11808.16, "test_speed_short": 2194.0}, {"test_speed": 12677.07, "test_speed_short": 2426.34}]}, "total": {"test_speed": 7915.157000000001, "test_speed_se": 2209.9507654335057, "test_speed_short": 1382.435, "test_speed_short_se": 461.34668777594527}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.10372195082903254, "macro_f1": 0.2891245111428598}, {"mcc": 0.24217248104679717, "macro_f1": 0.4299213991130437}, {"mcc": 0.03429813580051047, "macro_f1": 0.2318627450980392}, {"mcc": 0.07771750423344326, "macro_f1": 0.18345870374591597}, {"mcc": 0.201137479069156, "macro_f1": 0.34314466021783097}, {"mcc": 0.13318326686296464, "macro_f1": 0.33054954598677116}, {"mcc": 0.2788450006159256, "macro_f1": 0.49237102120007775}, {"mcc": 0.09873060357549425, "macro_f1": 0.3396184059329719}, {"mcc": 0.08797519751588977, "macro_f1": 0.23119868637110016}, {"mcc": 0.07909454841897283, "macro_f1": 0.30895132811300474}]}, "total": {"test_mcc": 13.368761679681866, "test_mcc_se": 4.958031902534744, "test_macro_f1": 31.802010069216152, "test_macro_f1_se": 5.775994206932882}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.14661654135338345, "micro_f1": 0.19834710743801653}, {"micro_f1_no_misc": 0.0049261083743842365, "micro_f1": 0.0861812778603269}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.04467353951890034}, {"micro_f1_no_misc": 0.06746987951807229, "micro_f1": 0.056451612903225805}, {"micro_f1_no_misc": 0.16216216216216214, "micro_f1": 0.14634146341463414}, {"micro_f1_no_misc": 0.18497109826589594, "micro_f1": 0.15136876006441222}, {"micro_f1_no_misc": 0.07766990291262135, "micro_f1": 0.06639004149377593}, {"micro_f1_no_misc": 0.08482142857142856, "micro_f1": 0.14804845222072677}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.1439509954058193}, {"micro_f1_no_misc": 0.10096153846153846, "micro_f1": 0.08984375}]}, "total": {"test_micro_f1_no_misc": 8.295986596194865, "test_micro_f1_no_misc_se": 4.192591373834753, "test_micro_f1": 11.315970003198379, "test_micro_f1_se": 3.156906128975784}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.03990596876004974, "macro_f1": 0.42329972108989483}, {"mcc": -0.04989615772799142, "macro_f1": 0.3644512668902913}, {"mcc": 0.0, "macro_f1": 0.3081081081081081}, {"mcc": 0.025352744740990613, "macro_f1": 0.5077453197347978}, {"mcc": 0.11585839547037943, "macro_f1": 0.5140859140859141}, {"mcc": -0.03970445566696172, "macro_f1": 0.42434684138154166}, {"mcc": 0.008931316628034687, "macro_f1": 0.4275885792913656}, {"mcc": 0.03941164060714018, "macro_f1": 0.5174030989930571}, {"mcc": -0.021408634078927936, "macro_f1": 0.3644512668902913}, {"mcc": 0.002672144834688232, "macro_f1": 0.3885565724672909}]}, "total": {"test_mcc": 0.4131102604730233, "test_mcc_se": 3.0400956960647214, "test_macro_f1": 42.400366889325525, "test_macro_f1_se": 4.414022130184473}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"em": 25.0, "f1": 28.455687830687825}, {"em": 26.875, "f1": 30.451388888888886}, {"em": 26.50602409638554, "f1": 31.033701907195873}, {"em": 29.166666666666668, "f1": 31.431878306878303}, {"em": 26.451612903225808, "f1": 29.569892473118276}, {"em": 11.176470588235293, "f1": 12.647058823529411}, {"em": 26.380368098159508, "f1": 31.509768044555024}, {"em": 18.012422360248447, "f1": 21.741102237996646}, {"em": 23.417721518987342, "f1": 26.464007160209693}, {"em": 24.848484848484848, "f1": 30.432086130087814}]}, "total": {"test_em": 23.783477108039346, "test_em_se": 3.297883585327548, "test_f1": 27.373657180314773, "test_f1_se": 3.702844098082199}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.5785667680902407, "rouge_l": 0.10478266684849649}, {"bertscore": 0.5366389472037554, "rouge_l": 0.0913718787126778}, {"bertscore": 0.5588699183426797, "rouge_l": 0.0592465602710938}, {"bertscore": 0.5264512610156089, "rouge_l": 0.10056308035402678}, {"bertscore": 0.5102915475727059, "rouge_l": 0.08544107986383509}, {"bertscore": 0.5299519307445735, "rouge_l": 0.0852840656238707}, {"bertscore": 0.5573177034966648, "rouge_l": 0.08020972095937073}, {"bertscore": 0.5413106380729005, "rouge_l": 0.09593393798762267}, {"bertscore": 0.5201394803007133, "rouge_l": 0.10335046660648749}, {"bertscore": 0.5671830219216645, "rouge_l": 0.10445701291820447}]}, "total": {"test_bertscore": 54.26721216761508, "test_bertscore_se": 1.3681781949241512, "test_rouge_l": 9.10640470145686, "test_rouge_l_se": 0.8832731310629042}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.028692677432506374, "accuracy": 0.28125}, {"mcc": -0.0024208992722143754, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.3046875}, {"mcc": 0.03698554815658994, "accuracy": 0.34375}, {"mcc": -0.024678211064752303, "accuracy": 0.29296875}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.025403295655385315, "accuracy": 0.33203125}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": -0.0008050402599027842, "accuracy": 0.25}]}, "total": {"test_mcc": 0.6317737064761216, "test_mcc_se": 1.1415423327908454, "test_accuracy": 29.531249999999996, "test_accuracy_se": 1.768871768972936}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.027267737597966975, "accuracy": 0.2421875}, {"mcc": -0.043750604530014926, "accuracy": 0.2734375}, {"mcc": 0.10755559175168214, "accuracy": 0.359375}, {"mcc": 0.0, "accuracy": 0.359375}, {"mcc": 0.09621545355176914, "accuracy": 0.3125}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.09214635435825942, "accuracy": 0.28125}, {"mcc": 0.11471021025308402, "accuracy": 0.375}, {"mcc": 0.139565504088289, "accuracy": 0.359375}, {"mcc": 0.005244787305023077, "accuracy": 0.359375}]}, "total": {"test_mcc": 4.84419559180125, "test_mcc_se": 4.192163287920145, "test_accuracy": 31.953125, "test_accuracy_se": 3.006263174203058}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.0, "macro_f1": 0.13991769547325103}, {"mcc": 0.0, "macro_f1": 0.16023738872403562}, {"mcc": 0.024774418294401634, "macro_f1": 0.2055286717703186}, {"mcc": 0.0, "macro_f1": 0.15568862275449102}, {"mcc": 0.0, "macro_f1": 0.17198067632850242}, {"mcc": 0.0, "macro_f1": 0.13664596273291926}, {"mcc": 0.0, "macro_f1": 0.13664596273291926}, {"mcc": 0.0, "macro_f1": 0.15415415415415415}, {"mcc": 0.028705520784502433, "macro_f1": 0.21596582163164013}, {"mcc": 0.0, "macro_f1": 0.13499480789200416}]}, "total": {"test_mcc": 0.5347993907890407, "test_mcc_se": 0.7011604066922986, "test_macro_f1": 16.11759764194236, "test_macro_f1_se": 1.7900545841413757}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.03378659359090625, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.10542575249516412, "accuracy": 0.296875}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": -0.07154608030212854, "accuracy": 0.22265625}, {"mcc": 0.029984866336036215, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": -0.019647296692061614, "accuracy": 0.2265625}]}, "total": {"test_mcc": 0.10430648246103925, "test_mcc_se": 2.8214011787979816, "test_accuracy": 24.9609375, "test_accuracy_se": 1.4546874999999997}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.02257336343115124, "micro_f1": 0.044293015332197615}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.015037593984962405}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.005291005291005291, "micro_f1": 0.0045662100456621}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.019138755980861243}, {"micro_f1_no_misc": 0.041176470588235294, "micro_f1": 0.04109589041095891}]}, "total": {"test_micro_f1_no_misc": 0.6904083931039182, "test_micro_f1_no_misc_se": 0.8658551154205193, "test_micro_f1": 1.241314657546423, "test_micro_f1_se": 1.0781292382920409}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 1826.3700000000001, "test_speed_short": 213.5}, {"test_speed": 3574.75, "test_speed_short": 422.75}, {"test_speed": 4886.13, "test_speed_short": 775.89}, {"test_speed": 6331.94, "test_speed_short": 939.78}, {"test_speed": 7387.379999999999, "test_speed_short": 1147.8500000000001}, {"test_speed": 8499.11, "test_speed_short": 1533.0}, {"test_speed": 9155.81, "test_speed_short": 1674.44}, {"test_speed": 10353.56, "test_speed_short": 1827.2799999999997}, {"test_speed": 11329.67, "test_speed_short": 2032.0}, {"test_speed": 11478.74, "test_speed_short": 2237.77}]}, "total": {"test_speed": 7482.3460000000005, "test_speed_se": 2047.1696063467464, "test_speed_short": 1280.426, "test_speed_short_se": 426.5007261138878}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.05597892954492245, "macro_f1": 0.4395583476108891}, {"mcc": 0.07882634225314346, "macro_f1": 0.4832806297517325}, {"mcc": -0.01616335513027424, "macro_f1": 0.36125710038753517}, {"mcc": -0.0912086009435502, "macro_f1": 0.33726520681265204}, {"mcc": 0.060057729003332144, "macro_f1": 0.35113468906572354}, {"mcc": -0.02887298234528568, "macro_f1": 0.36790123456790125}, {"mcc": -0.027187940356231143, "macro_f1": 0.3491235113073732}, {"mcc": 0.08896008544727063, "macro_f1": 0.39806159316396167}, {"mcc": 0.037004898065283946, "macro_f1": 0.515621394619041}, {"mcc": 0.0, "macro_f1": 0.36}]}, "total": {"test_mcc": 0.4543724644876646, "test_mcc_se": 3.7064253956158875, "test_macro_f1": 39.632037072868094, "test_macro_f1_se": 3.8530289303139154}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"em": 11.904761904761905, "f1": 16.909921584263685}, {"em": 18.75, "f1": 23.64710576259489}, {"em": 21.08433734939759, "f1": 25.977137477846192}, {"em": 16.071428571428573, "f1": 20.301174614531536}, {"em": 12.903225806451612, "f1": 17.918193986105702}, {"em": 18.823529411764707, "f1": 21.844948080242197}, {"em": 16.56441717791411, "f1": 20.80371154603334}, {"em": 9.937888198757763, "f1": 13.281497315658806}, {"em": 19.620253164556964, "f1": 23.489388931839066}, {"em": 10.303030303030303, "f1": 14.470826140972616}]}, "total": {"test_em": 15.596287188806354, "test_em_se": 2.519792535970748, "test_f1": 19.864390544008806, "test_f1_se": 2.570121686826835}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.48654443357372656, "rouge_l": 0.0770054255941777}, {"bertscore": 0.5089967101812363, "rouge_l": 0.08276018968503773}, {"bertscore": 0.48248690320178866, "rouge_l": 0.05913648107142949}, {"bertscore": 0.48192859173286706, "rouge_l": 0.07862994262793081}, {"bertscore": 0.5095930031966418, "rouge_l": 0.07775039913001365}, {"bertscore": 0.4682572314632125, "rouge_l": 0.05703117520175616}, {"bertscore": 0.5032303194748238, "rouge_l": 0.07904826545370841}, {"bertscore": 0.4719089058926329, "rouge_l": 0.06474196973565394}, {"bertscore": 0.4825666706310585, "rouge_l": 0.08366849513293367}, {"bertscore": 0.5271260933950543, "rouge_l": 0.06574575783356387}]}, "total": {"test_bertscore": 49.226388627430424, "test_bertscore_se": 1.1752234338625906, "test_rouge_l": 7.255181014662053, "test_rouge_l_se": 0.6135688651597584}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.017263422534464553, "accuracy": 0.24609375}, {"mcc": 0.13036318798426683, "accuracy": 0.3359375}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": 0.005009903345364105, "accuracy": 0.3046875}, {"mcc": 0.045982347726996234, "accuracy": 0.234375}, {"mcc": -0.06858022317765142, "accuracy": 0.2265625}, {"mcc": -0.001138877374839713, "accuracy": 0.2890625}, {"mcc": 0.07379717512888416, "accuracy": 0.3046875}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": 0.03339873925454003, "accuracy": 0.26953125}]}, "total": {"test_mcc": 2.0156883035309567, "test_mcc_se": 3.37180171755524, "test_accuracy": 26.914062500000004, "test_accuracy_se": 3.136986949323458}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.09180900593842722, "accuracy": 0.2265625}, {"mcc": -0.052108752337854884, "accuracy": 0.265625}, {"mcc": 0.1524065829467407, "accuracy": 0.375}, {"mcc": 0.0, "accuracy": 0.359375}, {"mcc": -0.08819548466396175, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": -0.08969724880451486, "accuracy": 0.2890625}, {"mcc": 0.0315501684982055, "accuracy": 0.2421875}, {"mcc": 0.00682581803109215, "accuracy": 0.359375}]}, "total": {"test_mcc": -1.310279222687204, "test_mcc_se": 4.597591450111877, "test_accuracy": 28.90625, "test_accuracy_se": 3.4163617750141415}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.05076563314591289, "accuracy": 0.265625}, {"mcc": -0.01404065858358923, "accuracy": 0.26171875}, {"mcc": 0.025769707038786015, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.006404104064463259, "accuracy": 0.2734375}, {"mcc": 0.002906107294259899, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": -0.0042522685170867675, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": -0.021148034671053955, "accuracy": 0.2265625}]}, "total": {"test_mcc": 0.4640458977169211, "test_mcc_se": 1.2631162132729876, "test_accuracy": 25.0, "test_accuracy_se": 0.9616986879056478}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 2010.19, "test_speed_short": 238.2}, {"test_speed": 3715.9300000000003, "test_speed_short": 429.59}, {"test_speed": 5170.679999999999, "test_speed_short": 847.67}, {"test_speed": 6577.419999999999, "test_speed_short": 1054.78}, {"test_speed": 7689.55, "test_speed_short": 1262.8}, {"test_speed": 8958.96, "test_speed_short": 1616.9499999999998}, {"test_speed": 10102.310000000001, "test_speed_short": 1811.3799999999999}, {"test_speed": 11016.88, "test_speed_short": 1978.34}, {"test_speed": 11929.810000000001, "test_speed_short": 2161.0}, {"test_speed": 12379.74, "test_speed_short": 2401.27}]}, "total": {"test_speed": 7955.147, "test_speed_se": 2200.1191057895467, "test_speed_short": 1380.1979999999999, "test_speed_short_se": 455.6063603952236}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": 0.1895010382667327, "macro_f1": 0.3459680416202155}, {"mcc": 0.0, "macro_f1": 0.14634146341463414}, {"mcc": 0.016393538667076008, "macro_f1": 0.2013355801036011}, {"mcc": 0.0312977394918434, "macro_f1": 0.18280472914619258}, {"mcc": 0.10359896155113996, "macro_f1": 0.2400141392718275}, {"mcc": 0.04733521401466858, "macro_f1": 0.26155100205188747}, {"mcc": 0.2750353262759327, "macro_f1": 0.3981487357747617}, {"mcc": 0.23677547887376169, "macro_f1": 0.39756224700932447}, {"mcc": 0.11029722593411163, "macro_f1": 0.24339610914289267}, {"mcc": 0.19037341366982544, "macro_f1": 0.43476607853012417}]}, "total": {"test_mcc": 12.006079367450921, "test_mcc_se": 6.062318634870192, "test_macro_f1": 28.51888126065461, "test_macro_f1_se": 6.286743409382926}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.05045871559633028, "micro_f1": 0.12138728323699423}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.06825938566552901}, {"micro_f1_no_misc": 0.130718954248366, "micro_f1": 0.11015490533562823}, {"micro_f1_no_misc": 0.15817223198594024, "micro_f1": 0.15007656967840735}, {"micro_f1_no_misc": 0.1727115716753022, "micro_f1": 0.159375}, {"micro_f1_no_misc": 0.1761252446183953, "micro_f1": 0.1775147928994083}, {"micro_f1_no_misc": 0.05454545454545455, "micro_f1": 0.08119079837618402}, {"micro_f1_no_misc": 0.010958904109589041, "micro_f1": 0.13043478260869565}, {"micro_f1_no_misc": 0.06806282722513089, "micro_f1": 0.08695652173913045}, {"micro_f1_no_misc": 0.09619238476953906, "micro_f1": 0.12852664576802508}]}, "total": {"test_micro_f1_no_misc": 9.179462887740476, "test_micro_f1_no_misc_se": 4.0390406365771705, "test_micro_f1": 12.138766853080023, "test_micro_f1_se": 2.196137159185634}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.03562016834586698, "macro_f1": 0.44563234266032364}, {"mcc": -0.027874733666903025, "macro_f1": 0.3871086556169429}, {"mcc": 0.05647029422919835, "macro_f1": 0.40015620932048945}, {"mcc": -0.013906241888090465, "macro_f1": 0.48074642701117143}, {"mcc": 0.07132666163512592, "macro_f1": 0.48055313997305915}, {"mcc": 0.008104408984731078, "macro_f1": 0.4890690248096291}, {"mcc": -0.022628141110071023, "macro_f1": 0.473972602739726}, {"mcc": -0.0657951694959769, "macro_f1": 0.45545545545545546}, {"mcc": -0.08177121474473219, "macro_f1": 0.4139889783806698}, {"mcc": -0.0020419179497035973, "macro_f1": 0.4964147471286249}]}, "total": {"test_mcc": -1.1373622235228882, "test_mcc_se": 2.981047259576095, "test_macro_f1": 45.23097583096092, "test_macro_f1_se": 2.432844134899824}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"em": 22.023809523809526, "f1": 25.23411411348165}, {"em": 25.0, "f1": 27.833088235294117}, {"em": 22.89156626506024, "f1": 26.392453735976435}, {"em": 25.0, "f1": 27.65411720243653}, {"em": 26.451612903225808, "f1": 29.95959595959596}, {"em": 8.823529411764707, "f1": 10.980392156862743}, {"em": 22.085889570552148, "f1": 25.18694185240558}, {"em": 8.074534161490684, "f1": 10.718730179818964}, {"em": 22.78481012658228, "f1": 24.72412133246087}, {"em": 24.242424242424242, "f1": 28.77685950413223}]}, "total": {"test_em": 20.737817620490965, "test_em_se": 4.110661072921973, "test_f1": 23.746041427246507, "test_f1_se": 4.337748256533314}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"bertscore": 0.5735158929601312, "rouge_l": 0.09363722544452742}, {"bertscore": 0.5522622860735282, "rouge_l": 0.09576365602950926}, {"bertscore": 0.5339416596107185, "rouge_l": 0.07403693582292334}, {"bertscore": 0.40095713490154594, "rouge_l": 0.05903110219872407}, {"bertscore": 0.5286671596113592, "rouge_l": 0.08727901863772503}, {"bertscore": 0.542712367605418, "rouge_l": 0.1054215036341577}, {"bertscore": 0.5534800859168172, "rouge_l": 0.09607598257166161}, {"bertscore": 0.5356817739084363, "rouge_l": 0.08729621616678454}, {"bertscore": 0.4689402508083731, "rouge_l": 0.08808590224853824}, {"bertscore": 0.4642398958094418, "rouge_l": 0.08239258515060766}]}, "total": {"test_bertscore": 51.543985072057694, "test_bertscore_se": 3.3144368576512844, "test_rouge_l": 8.690201279051589, "test_rouge_l_se": 0.8042522711325437}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.027529321784758448, "accuracy": 0.21875}, {"mcc": -0.04669612484854005, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": 0.04679012907338442, "accuracy": 0.28515625}, {"mcc": 0.04512502380174535, "accuracy": 0.21875}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": 0.0019172153348755293, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.006817420359078263, "accuracy": 0.3125}, {"mcc": -0.04881773382197126, "accuracy": 0.25390625}]}, "total": {"test_mcc": -0.22393391886186184, "test_mcc_se": 2.0186999102456875, "test_accuracy": 25.156250000000004, "test_accuracy_se": 2.59058458117511}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.19507955985368938, "accuracy": 0.234375}, {"mcc": -0.19406941633534222, "accuracy": 0.265625}, {"mcc": -0.10369321913471968, "accuracy": 0.25}, {"mcc": -0.1188706955048525, "accuracy": 0.296875}, {"mcc": 0.005899956582379265, "accuracy": 0.140625}, {"mcc": -0.05068805024645956, "accuracy": 0.1875}, {"mcc": 0.03365437088438325, "accuracy": 0.3984375}, {"mcc": -0.09857691763803396, "accuracy": 0.28125}, {"mcc": 0.0569747556224614, "accuracy": 0.4140625}, {"mcc": -0.12469957203284184, "accuracy": 0.2734375}]}, "total": {"test_mcc": -7.891483476567155, "test_mcc_se": 5.488630230293493, "test_accuracy": 27.421875, "test_accuracy_se": 5.192470531164705}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": 0.021168470798591337, "accuracy": 0.265625}, {"mcc": -0.07408025685992116, "accuracy": 0.24609375}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": -0.005501753672547956, "accuracy": 0.25}, {"mcc": 0.07290316944026093, "accuracy": 0.23046875}, {"mcc": -0.033441610497727405, "accuracy": 0.2265625}, {"mcc": 0.007692632353413104, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.011900748440182825, "accuracy": 0.23828125}, {"mcc": -0.08940568182286265, "accuracy": 0.25}]}, "total": {"test_mcc": -0.8876428182061098, "test_mcc_se": 2.9075713063852713, "test_accuracy": 25.0, "test_accuracy_se": 1.1065569646304805}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"test_speed": 2045.68, "test_speed_short": 236.29999999999998}, {"test_speed": 3795.5699999999997, "test_speed_short": 448.78000000000003}, {"test_speed": 5249.27, "test_speed_short": 868.39}, {"test_speed": 6707.379999999999, "test_speed_short": 1079.16}, {"test_speed": 7856.420000000001, "test_speed_short": 1245.75}, {"test_speed": 9126.67, "test_speed_short": 1669.51}, {"test_speed": 10190.65, "test_speed_short": 1827.78}, {"test_speed": 11247.6, "test_speed_short": 2039.31}, {"test_speed": 12083.9, "test_speed_short": 2162.0}, {"test_speed": 12505.880000000001, "test_speed_short": 2383.83}]}, "total": {"test_speed": 8080.902, "test_speed_se": 2224.4070272871263, "test_speed_short": 1396.081, "test_speed_short_se": 456.45133527327454}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": 0.0, "macro_f1": 0.13991769547325103}, {"mcc": 0.0, "macro_f1": 0.16023738872403562}, {"mcc": 0.15860462504954284, "macro_f1": 0.3080481036077706}, {"mcc": 0.0, "macro_f1": 0.15568862275449102}, {"mcc": 0.0, "macro_f1": 0.17198067632850242}, {"mcc": 0.0, "macro_f1": 0.13664596273291926}, {"mcc": 0.0, "macro_f1": 0.13664596273291926}, {"mcc": 0.0, "macro_f1": 0.15415415415415415}, {"mcc": 0.11157691294249242, "macro_f1": 0.2974823677766536}, {"mcc": 0.0, "macro_f1": 0.13499480789200416}]}, "total": {"test_mcc": 2.701815379920353, "test_mcc_se": 3.596600192362954, "test_macro_f1": 17.957957421767013, "test_macro_f1_se": 4.0958206447581915}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.015113350125944584, "micro_f1": 0.012072434607645876}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.005376344086021506, "micro_f1": 0.004629629629629629}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.017964071856287425, "micro_f1": 0.013953488372093025}]}, "total": {"test_micro_f1_no_misc": 0.38453766068253514, "test_micro_f1_no_misc_se": 0.4294925543553871, "test_micro_f1": 0.3065555260936853, "test_micro_f1_se": 0.3381596122225282}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.06385866795930598, "macro_f1": 0.3521583192236969}, {"mcc": 0.032610622139970014, "macro_f1": 0.44458908270415687}, {"mcc": 0.024144971829946942, "macro_f1": 0.4287131167865113}, {"mcc": 0.0, "macro_f1": 0.33159268929503916}, {"mcc": 0.060057729003332144, "macro_f1": 0.35113468906572354}, {"mcc": -0.041643092527110404, "macro_f1": 0.3216950108052241}, {"mcc": 0.0, "macro_f1": 0.3263157894736842}, {"mcc": -0.08873565094161139, "macro_f1": 0.3298429319371728}, {"mcc": 0.0630770306871049, "macro_f1": 0.4518589301197997}, {"mcc": 0.0, "macro_f1": 0.36}]}, "total": {"test_mcc": -0.14347057767673754, "test_mcc_se": 3.126292743591605, "test_macro_f1": 36.979005594110085, "test_macro_f1_se": 3.1882950763444042}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"em": 12.5, "f1": 16.784283157707527}, {"em": 16.875, "f1": 20.66539881213794}, {"em": 21.08433734939759, "f1": 25.679972513591817}, {"em": 16.071428571428573, "f1": 19.72392618663627}, {"em": 12.903225806451612, "f1": 17.268767012400286}, {"em": 18.823529411764707, "f1": 21.315281668222845}, {"em": 18.404907975460123, "f1": 22.25900558036021}, {"em": 11.801242236024844, "f1": 14.345401138057328}, {"em": 22.151898734177216, "f1": 25.371709865380748}, {"em": 10.303030303030303, "f1": 14.192236345927654}]}, "total": {"test_em": 16.091860038773497, "test_em_se": 2.5308857521605925, "test_f1": 19.760598228042262, "test_f1_se": 2.542141260115786}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"bertscore": 0.49991326802410185, "rouge_l": 0.07709695047244536}, {"bertscore": 0.5018724026158452, "rouge_l": 0.08172885916455278}, {"bertscore": 0.49087451735977083, "rouge_l": 0.0644776289633566}, {"bertscore": 0.48389116500038654, "rouge_l": 0.07726185526338754}, {"bertscore": 0.5144481214229017, "rouge_l": 0.08635617771638349}, {"bertscore": 0.5013242381392047, "rouge_l": 0.07401536043904938}, {"bertscore": 0.5064756735228002, "rouge_l": 0.08651205807509027}, {"bertscore": 0.47294020315166563, "rouge_l": 0.06921104391858003}, {"bertscore": 0.4832014818675816, "rouge_l": 0.08638774901030899}, {"bertscore": 0.5462348649743944, "rouge_l": 0.06914425060759541}]}, "total": {"test_bertscore": 50.01175936078652, "test_bertscore_se": 1.265286877458781, "test_rouge_l": 7.721919336307499, "test_rouge_l_se": 0.495933471492669}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.029866944905742317, "accuracy": 0.234375}, {"mcc": 0.09025996099213544, "accuracy": 0.30078125}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": 0.014261599878279768, "accuracy": 0.3046875}, {"mcc": 0.032749978701966574, "accuracy": 0.2265625}, {"mcc": -0.05023773265984299, "accuracy": 0.23828125}, {"mcc": 0.0048787970133350855, "accuracy": 0.2890625}, {"mcc": 0.08826066433815039, "accuracy": 0.3046875}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": 0.01950994068699461, "accuracy": 0.26171875}]}, "total": {"test_mcc": 1.6981626404527654, "test_mcc_se": 2.7833309731563274, "test_accuracy": 26.406249999999996, "test_accuracy_se": 2.90803148803734}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.01985490253767404, "accuracy": 0.234375}, {"mcc": -0.02103119853637755, "accuracy": 0.2890625}, {"mcc": 0.16170374759097095, "accuracy": 0.390625}, {"mcc": 0.0, "accuracy": 0.359375}, {"mcc": -0.14603920228011197, "accuracy": 0.203125}, {"mcc": 0.08761702229637969, "accuracy": 0.3046875}, {"mcc": 0.11134328776689689, "accuracy": 0.28125}, {"mcc": -0.09536213503671799, "accuracy": 0.3046875}, {"mcc": 0.07764137110104874, "accuracy": 0.2578125}, {"mcc": 0.1458735864207542, "accuracy": 0.40625}]}, "total": {"test_mcc": 3.018915767851689, "test_mcc_se": 6.390438973494721, "test_accuracy": 30.312499999999996, "test_accuracy_se": 4.07566989210908}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": 0.05076563314591289, "accuracy": 0.265625}, {"mcc": -0.0038220007148562565, "accuracy": 0.265625}, {"mcc": 0.044705181270010295, "accuracy": 0.265625}, {"mcc": -0.022777802691078406, "accuracy": 0.2421875}, {"mcc": 0.0027565233693795633, "accuracy": 0.265625}, {"mcc": 0.02602294804131901, "accuracy": 0.265625}, {"mcc": -0.04511345153232753, "accuracy": 0.24609375}, {"mcc": -0.026493437329769092, "accuracy": 0.2265625}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": -0.012903773244608488, "accuracy": 0.23046875}]}, "total": {"test_mcc": 0.1313982031398198, "test_mcc_se": 1.9248733008791894, "test_accuracy": 25.078125, "test_accuracy_se": 1.0259248029894241}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"test_speed": 1821.82, "test_speed_short": 212.2}, {"test_speed": 3437.1899999999996, "test_speed_short": 409.45}, {"test_speed": 4775.02, "test_speed_short": 777.0}, {"test_speed": 5949.28, "test_speed_short": 949.9}, {"test_speed": 7058.150000000001, "test_speed_short": 1140.6999999999998}, {"test_speed": 8401.73, "test_speed_short": 1468.03}, {"test_speed": 9193.67, "test_speed_short": 1717.8999999999999}, {"test_speed": 9949.800000000001, "test_speed_short": 1814.5400000000002}, {"test_speed": 11426.99, "test_speed_short": 2060.0}, {"test_speed": 11514.779999999999, "test_speed_short": 2215.97}]}, "total": {"test_speed": 7352.842999999999, "test_speed_se": 2063.7469644465086, "test_speed_short": 1276.569, "test_speed_short_se": 427.2520510188448}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"mcc": 0.36764706917469747, "macro_f1": 0.452426984779926}, {"mcc": 0.29982380373964224, "macro_f1": 0.38326020464131627}, {"mcc": 0.36874516461429924, "macro_f1": 0.5546622994287497}, {"mcc": 0.35030068883898313, "macro_f1": 0.4229568721034793}, {"mcc": 0.37479494160442023, "macro_f1": 0.4498193881707291}, {"mcc": 0.23360196697552593, "macro_f1": 0.36677086293454586}, {"mcc": 0.28951631778035836, "macro_f1": 0.4134032588417935}, {"mcc": 0.3946382483636516, "macro_f1": 0.452192378328742}, {"mcc": 0.3553304807704582, "macro_f1": 0.43729975822999084}, {"mcc": 0.41405013547358155, "macro_f1": 0.4516339869281046}]}, "total": {"test_mcc": 34.484488173356176, "test_mcc_se": 3.3859918228660253, "test_macro_f1": 43.84425994387377, "test_macro_f1_se": 3.1499272051606693}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024_with_steering", "results": {"raw": {"test": [{"mcc": -0.014247871535525097, "macro_f1": 0.22760639139376682}, {"mcc": 0.2687021553231885, "macro_f1": 0.4050905039236595}, {"mcc": 0.1138216978053743, "macro_f1": 0.2193012798339675}, {"mcc": 0.06262129889297381, "macro_f1": 0.1769226357745025}, {"mcc": 0.15990717268132318, "macro_f1": 0.32228089370946517}, {"mcc": 0.11634733103175315, "macro_f1": 0.32229682087127137}, {"mcc": 0.13332749631349314, "macro_f1": 0.31301092827213983}, {"mcc": 0.15955207020221135, "macro_f1": 0.389220915536705}, {"mcc": 0.10613773631772103, "macro_f1": 0.24057040998217472}, {"mcc": 0.06735212970627884, "macro_f1": 0.306752055660974}]}, "total": {"test_mcc": 11.735212167387921, "test_mcc_se": 4.606211731206913, "test_macro_f1": 29.230528349586265, "test_macro_f1_se": 4.622248693654778}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.2686084142394822, "micro_f1": 0.24157303370786515}, {"micro_f1_no_misc": 0.15530303030303028, "micro_f1": 0.13245033112582782}, {"micro_f1_no_misc": 0.006802721088435374, "micro_f1": 0.0421792618629174}, {"micro_f1_no_misc": 0.1851851851851852, "micro_f1": 0.1693121693121693}, {"micro_f1_no_misc": 0.1114864864864865, "micro_f1": 0.10429447852760736}, {"micro_f1_no_misc": 0.14371257485029942, "micro_f1": 0.13289036544850497}, {"micro_f1_no_misc": 0.14360770577933452, "micro_f1": 0.13416536661466458}, {"micro_f1_no_misc": 0.11669367909238249, "micro_f1": 0.11436541143654114}, {"micro_f1_no_misc": 0.15979381443298968, "micro_f1": 0.12832550860719877}, {"micro_f1_no_misc": 0.13636363636363635, "micro_f1": 0.1497005988023952}]}, "total": {"test_micro_f1_no_misc": 14.27557247821262, "test_micro_f1_no_misc_se": 4.0416788869421385, "test_micro_f1": 13.492565254456917, "test_micro_f1_se": 3.119055370427818}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"mcc": 0.0010415395333694742, "macro_f1": 0.3659995230320486}, {"mcc": -0.06845986232735529, "macro_f1": 0.4202023252302582}, {"mcc": 0.02654118980044986, "macro_f1": 0.37693121693121695}, {"mcc": 0.02783771817922497, "macro_f1": 0.4484268125854993}, {"mcc": 0.08561667029818171, "macro_f1": 0.3729241807626285}, {"mcc": -0.03930076017125931, "macro_f1": 0.33782172405569516}, {"mcc": -0.013053462764050698, "macro_f1": 0.41878224974200207}, {"mcc": 0.051639777949432225, "macro_f1": 0.36318407960199}, {"mcc": 0.0925853915361429, "macro_f1": 0.4025498100179374}, {"mcc": -0.05588366139390376, "macro_f1": 0.3758722152164775}]}, "total": {"test_mcc": 1.0856454064023209, "test_mcc_se": 3.4814420447088814, "test_macro_f1": 38.826941371757535, "test_macro_f1_se": 2.0620403931825373}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"em": 37.5, "f1": 41.17623117623117}, {"em": 39.375, "f1": 42.25885225885226}, {"em": 34.33734939759036, "f1": 38.051296093464764}, {"em": 35.714285714285715, "f1": 39.90551776266062}, {"em": 41.935483870967744, "f1": 46.53353814644137}, {"em": 36.470588235294116, "f1": 41.33478492611619}, {"em": 34.969325153374236, "f1": 38.500340831629174}, {"em": 27.950310559006212, "f1": 31.758772628337837}, {"em": 32.91139240506329, "f1": 37.67229254571026}, {"em": 35.15151515151515, "f1": 40.689317416590136}]}, "total": {"test_em": 35.631525048709676, "test_em_se": 2.3255322956345164, "test_f1": 39.78809437860338, "test_f1_se": 2.3589968233419425}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"bertscore": 0.5671633989550173, "rouge_l": 0.11272261603848151}, {"bertscore": 0.5767329749651253, "rouge_l": 0.11889168546202233}, {"bertscore": 0.5657548937015235, "rouge_l": 0.10166649998027676}, {"bertscore": 0.6295424109557644, "rouge_l": 0.14646976802908374}, {"bertscore": 0.5821040356531739, "rouge_l": 0.121269347975734}, {"bertscore": 0.6233890349976718, "rouge_l": 0.14316715248202494}, {"bertscore": 0.5721796227153391, "rouge_l": 0.10575761630678451}, {"bertscore": 0.5940379217499867, "rouge_l": 0.1267683806593959}, {"bertscore": 0.5992832910269499, "rouge_l": 0.13428864178378944}, {"bertscore": 0.6265145275974646, "rouge_l": 0.14711956224872574}]}, "total": {"test_bertscore": 59.367021123180166, "test_bertscore_se": 1.5522383348901785, "test_rouge_l": 12.58121270966319, "test_rouge_l_se": 1.0288220116418232}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": -0.029035985847284472, "accuracy": 0.265625}, {"mcc": 0.00882811759897363, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.3046875}, {"mcc": 0.10088634083907931, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.296875}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.328125}, {"mcc": 0.07191679071909113, "accuracy": 0.328125}, {"mcc": 0.03191958514001756, "accuracy": 0.26953125}]}, "total": {"test_mcc": 1.8451484844987716, "test_mcc_se": 2.434522836589078, "test_accuracy": 29.9609375, "test_accuracy_se": 1.7683193705514175}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"mcc": 0.023020868556927185, "accuracy": 0.3359375}, {"mcc": 0.13701688119248884, "accuracy": 0.4296875}, {"mcc": 0.11469464011865305, "accuracy": 0.3671875}, {"mcc": 0.027291579380777167, "accuracy": 0.375}, {"mcc": 0.03676462475155315, "accuracy": 0.3125}, {"mcc": 0.07878340693893382, "accuracy": 0.3515625}, {"mcc": -0.0938954146352917, "accuracy": 0.359375}, {"mcc": 0.11324921237062129, "accuracy": 0.3984375}, {"mcc": 0.008821799992616855, "accuracy": 0.28125}, {"mcc": 0.09560769561521454, "accuracy": 0.3984375}]}, "total": {"test_mcc": 5.413552942824942, "test_mcc_se": 4.24996084672277, "test_accuracy": 36.09375, "test_accuracy_se": 2.7085769121235765}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": 0.03827476197880848, "accuracy": 0.23828125}]}, "total": {"test_mcc": 0.3827476197880848, "test_mcc_se": 0.7501853347846462, "test_accuracy": 25.234374999999996, "test_accuracy_se": 1.0092836215947647}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_sw3_356m_with_steering", "results": {"raw": {"test": [{"test_speed": 2023.84, "test_speed_short": 232.89999999999998}, {"test_speed": 3766.6099999999997, "test_speed_short": 444.79}, {"test_speed": 5181.52, "test_speed_short": 879.86}, {"test_speed": 6570.2, "test_speed_short": 1041.44}, {"test_speed": 7797.79, "test_speed_short": 1249.05}, {"test_speed": 8877.81, "test_speed_short": 1602.35}, {"test_speed": 10146.48, "test_speed_short": 1802.3600000000001}, {"test_speed": 11074.56, "test_speed_short": 1951.0400000000002}, {"test_speed": 12278.54, "test_speed_short": 2192.0}, {"test_speed": 12478.85, "test_speed_short": 2402.36}]}, "total": {"test_speed": 8019.620000000001, "test_speed_se": 2233.388480408974, "test_speed_short": 1379.815, "test_speed_short_se": 454.0946205224115}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}