
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.35518607021558046, "macro_f1": 0.43526041053416537}, {"mcc": 0.27918473011692274, "macro_f1": 0.37762957123088375}, {"mcc": 0.37229822078705205, "macro_f1": 0.5633428015916495}, {"mcc": 0.3366718281433893, "macro_f1": 0.40391143537518365}, {"mcc": 0.37457968221213434, "macro_f1": 0.44969135802469135}, {"mcc": 0.2097969770636337, "macro_f1": 0.355657962109575}, {"mcc": 0.30410057061104384, "macro_f1": 0.42119992342631196}, {"mcc": 0.3946382483636516, "macro_f1": 0.452192378328742}, {"mcc": 0.3457724202803098, "macro_f1": 0.43257534292017047}, {"mcc": 0.4318769956363507, "macro_f1": 0.4599063636905827}]}, "total": {"test_mcc": 34.04105743430069, "test_mcc_se": 3.9134105953203524, "test_macro_f1": 43.513675472319555, "test_macro_f1_se": 3.478829229847264}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.23856209150326796, "micro_f1": 0.21529745042492918}, {"micro_f1_no_misc": 0.14942528735632185, "micro_f1": 0.13666666666666666}, {"micro_f1_no_misc": 0.006802721088435374, "micro_f1": 0.03985507246376812}, {"micro_f1_no_misc": 0.1900647948164147, "micro_f1": 0.16544117647058823}, {"micro_f1_no_misc": 0.1254125412541254, "micro_f1": 0.12012012012012012}, {"micro_f1_no_misc": 0.14545454545454548, "micro_f1": 0.12751677852348994}, {"micro_f1_no_misc": 0.15827338129496404, "micro_f1": 0.1437699680511182}, {"micro_f1_no_misc": 0.11960132890365448, "micro_f1": 0.11680911680911683}, {"micro_f1_no_misc": 0.1757105943152455, "micro_f1": 0.1320450885668277}, {"micro_f1_no_misc": 0.12802768166089964, "micro_f1": 0.1394658753709199}]}, "total": {"test_micro_f1_no_misc": 14.373349676478744, "test_micro_f1_no_misc_se": 3.714421618176756, "test_micro_f1": 13.36987313467545, "test_micro_f1_se": 2.7037782498405227}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.030455672411361313, "macro_f1": 0.3801227083219546}, {"mcc": 0.0, "macro_f1": 0.46440013076168685}, {"mcc": -0.020828752216792853, "macro_f1": 0.36955724390882894}, {"mcc": -0.003944481036418488, "macro_f1": 0.4487823605014104}, {"mcc": 0.08561667029818171, "macro_f1": 0.3729241807626285}, {"mcc": -0.0872797785719991, "macro_f1": 0.34091396695987985}, {"mcc": -0.02430938013719691, "macro_f1": 0.419441744675772}, {"mcc": 0.051639777949432225, "macro_f1": 0.36318407960199}, {"mcc": 0.10685243369373369, "macro_f1": 0.41013824884792627}, {"mcc": -0.05588366139390376, "macro_f1": 0.3758722152164775}]}, "total": {"test_mcc": 0.8231850099639784, "test_mcc_se": 3.778352861821553, "test_macro_f1": 39.453368795585554, "test_macro_f1_se": 2.4613196189979685}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"em": 38.69047619047619, "f1": 42.465913715913715}, {"em": 40.0, "f1": 43.38385225885226}, {"em": 34.93975903614458, "f1": 38.97499087258123}, {"em": 36.30952380952381, "f1": 40.50075585789872}, {"em": 40.645161290322584, "f1": 45.5657962109575}, {"em": 37.64705882352941, "f1": 42.58349493643611}, {"em": 34.355828220858896, "f1": 37.88684389911384}, {"em": 26.70807453416149, "f1": 30.871461182020184}, {"em": 33.54430379746835, "f1": 38.600562587904356}, {"em": 35.15151515151515, "f1": 40.20446893174166}]}, "total": {"test_em": 35.799170085400036, "test_em_se": 2.480423066762068, "test_f1": 40.103814045341956, "test_f1_se": 2.497863701454097}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.14136913641191975, "macro_f1": 0.3114638737471493}, {"mcc": 0.0, "macro_f1": 0.14634146341463414}, {"mcc": 0.03534319192453339, "macro_f1": 0.22649616022192906}, {"mcc": 0.08559226427019176, "macro_f1": 0.21515953367080842}, {"mcc": 0.10359896155113996, "macro_f1": 0.2400141392718275}, {"mcc": 0.12125734759442537, "macro_f1": 0.3411540214171793}, {"mcc": 0.31160394248745316, "macro_f1": 0.3956073708341205}, {"mcc": 0.2536962249684162, "macro_f1": 0.4224724372507624}, {"mcc": 0.09017524047903357, "macro_f1": 0.22783389450056116}, {"mcc": 0.1729435045194054, "macro_f1": 0.41152443431925945}]}, "total": {"test_mcc": 13.155798142065183, "test_mcc_se": 5.858957513075483, "test_macro_f1": 29.380673286482317, "test_macro_f1_se": 5.953490466921334}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.02824858757062147, "micro_f1": 0.05614035087719299}, {"micro_f1_no_misc": 0.01038961038961039, "micro_f1": 0.01553398058252427}, {"micro_f1_no_misc": 0.08421052631578947, "micro_f1": 0.08805031446540881}, {"micro_f1_no_misc": 0.09130434782608694, "micro_f1": 0.08872458410351203}, {"micro_f1_no_misc": 0.12788632326820604, "micro_f1": 0.12520064205457465}, {"micro_f1_no_misc": 0.0915032679738562, "micro_f1": 0.13043478260869565}, {"micro_f1_no_misc": 0.07615230460921844, "micro_f1": 0.07704160246533127}, {"micro_f1_no_misc": 0.011080332409972301, "micro_f1": 0.1296551724137931}, {"micro_f1_no_misc": 0.053571428571428575, "micro_f1": 0.04245283018867924}, {"micro_f1_no_misc": 0.10993657505285413, "micro_f1": 0.11447811447811447}]}, "total": {"test_micro_f1_no_misc": 6.8428330398764405, "test_micro_f1_no_misc_se": 2.5400399213334115, "test_micro_f1": 8.677123742378265, "test_micro_f1_se": 2.4504149469700907}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"bertscore": 0.5768308055121452, "rouge_l": 0.11530059474170634}, {"bertscore": 0.5749611138598993, "rouge_l": 0.1167863073987071}, {"bertscore": 0.5708496569423005, "rouge_l": 0.09917787148676435}, {"bertscore": 0.628724463400431, "rouge_l": 0.14530517526538414}, {"bertscore": 0.5823411884484813, "rouge_l": 0.12025151260666536}, {"bertscore": 0.6220454624854028, "rouge_l": 0.14332318012335116}, {"bertscore": 0.5721241178689525, "rouge_l": 0.10384514701336606}, {"bertscore": 0.5979103631107137, "rouge_l": 0.12629304877231762}, {"bertscore": 0.597724124090746, "rouge_l": 0.13625657396434515}, {"bertscore": 0.6268335273489356, "rouge_l": 0.15114196714475564}]}, "total": {"test_bertscore": 59.50344823068008, "test_bertscore_se": 1.4457007860873563, "test_rouge_l": 12.576813785173629, "test_rouge_l_se": 1.1034034193295614}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.05115465928055023, "macro_f1": 0.44305590690614727}, {"mcc": -0.04706497621604712, "macro_f1": 0.3698017476275486}, {"mcc": 0.01800250112619768, "macro_f1": 0.36600922906978817}, {"mcc": 0.011380752593945711, "macro_f1": 0.48808867755192015}, {"mcc": 0.10321442750728609, "macro_f1": 0.4892328861369109}, {"mcc": 0.009665607189440706, "macro_f1": 0.48840927258193445}, {"mcc": -0.023417157276098777, "macro_f1": 0.47619047619047616}, {"mcc": -0.11514154661795957, "macro_f1": 0.4314314314314315}, {"mcc": -0.13026411630938972, "macro_f1": 0.393939393939394}, {"mcc": -0.016265001215808886, "macro_f1": 0.4898785425101214}]}, "total": {"test_mcc": -2.410441684989841, "test_mcc_se": 4.196677138723446, "test_macro_f1": 44.36037563945673, "test_macro_f1_se": 3.1573222611431686}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": -0.028384140682101006, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.3046875}, {"mcc": 0.10088634083907931, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.296875}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.328125}, {"mcc": 0.0747959184681732, "accuracy": 0.328125}, {"mcc": 0.024919897531243386, "accuracy": 0.265625}]}, "total": {"test_mcc": 1.722180161563949, "test_mcc_se": 2.464828426118755, "test_accuracy": 29.921874999999996, "test_accuracy_se": 1.7980871515346213}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.03046480879906189, "accuracy": 0.3359375}, {"mcc": 0.12131496817443596, "accuracy": 0.4140625}, {"mcc": 0.11469464011865305, "accuracy": 0.3671875}, {"mcc": 0.04726310723560305, "accuracy": 0.3828125}, {"mcc": -0.08445739633771641, "accuracy": 0.296875}, {"mcc": 0.07878340693893382, "accuracy": 0.3515625}, {"mcc": -0.0938954146352917, "accuracy": 0.359375}, {"mcc": 0.08705309281450903, "accuracy": 0.3828125}, {"mcc": 0.029052065519573258, "accuracy": 0.2890625}, {"mcc": 0.08230413460886296, "accuracy": 0.390625}]}, "total": {"test_mcc": 4.1257741323662485, "test_mcc_se": 4.682378313165761, "test_accuracy": 35.703125, "test_accuracy_se": 2.4906032168293195}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"em": 21.428571428571427, "f1": 25.255372616883008}, {"em": 26.25, "f1": 29.32854278074867}, {"em": 23.49397590361446, "f1": 26.606694156304357}, {"em": 25.595238095238095, "f1": 28.546974345293673}, {"em": 25.806451612903224, "f1": 29.744542196155102}, {"em": 8.823529411764707, "f1": 10.980392156862743}, {"em": 22.085889570552148, "f1": 24.696144306393315}, {"em": 8.074534161490684, "f1": 10.23070888434425}, {"em": 20.88607594936709, "f1": 22.825387155245682}, {"em": 23.03030303030303, "f1": 27.059687786960517}]}, "total": {"test_em": 20.547456916380487, "test_em_se": 4.115402081298903, "test_f1": 23.527444638519135, "test_f1_se": 4.424062602425179}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": 0.03827476197880848, "accuracy": 0.23828125}]}, "total": {"test_mcc": 0.3827476197880848, "test_mcc_se": 0.7501853347846462, "test_accuracy": 25.234374999999996, "test_accuracy_se": 1.0092836215947647}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"test_speed": 2189.46, "test_speed_short": 291.3}, {"test_speed": 3813.67, "test_speed_short": 405.65000000000003}, {"test_speed": 6571.75, "test_speed_short": 1128.8700000000001}, {"test_speed": 8154.99, "test_speed_short": 1388.28}, {"test_speed": 9714.539999999999, "test_speed_short": 1640.1}, {"test_speed": 11128.37, "test_speed_short": 2086.3399999999997}, {"test_speed": 10569.25, "test_speed_short": 1923.72}, {"test_speed": 13266.4, "test_speed_short": 2616.25}, {"test_speed": 15133.26, "test_speed_short": 2868.0}, {"test_speed": 15560.27, "test_speed_short": 3030.2000000000003}]}, "total": {"test_speed": 9610.196, "test_speed_se": 2791.2536696756856, "test_speed_short": 1737.8709999999999, "test_speed_short_se": 593.2346414490478}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.5760471230605617, "rouge_l": 0.09705440079024233}, {"bertscore": 0.5524321855045855, "rouge_l": 0.09902130289160324}, {"bertscore": 0.5577755308477208, "rouge_l": 0.07976092448376546}, {"bertscore": 0.4145923152100295, "rouge_l": 0.06110526104327817}, {"bertscore": 0.5287695411825553, "rouge_l": 0.08861783501955167}, {"bertscore": 0.5560010847402737, "rouge_l": 0.10270217779267202}, {"bertscore": 0.5504610406933352, "rouge_l": 0.08959344005072686}, {"bertscore": 0.5459430078044534, "rouge_l": 0.08978267818001737}, {"bertscore": 0.5001479594502598, "rouge_l": 0.09506851106101627}, {"bertscore": 0.48966777918394655, "rouge_l": 0.08889251091063122}]}, "total": {"test_bertscore": 52.71837567677722, "test_bertscore_se": 2.958300153307908, "test_rouge_l": 8.915990422235046, "test_rouge_l_se": 0.7316808404786563}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.02269037410444548, "accuracy": 0.23046875}, {"mcc": -0.04669612484854005, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": -0.0349143066109712, "accuracy": 0.25390625}, {"mcc": 0.04512502380174535, "accuracy": 0.21875}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": -0.005908121662595136, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": -0.04881773382197126, "accuracy": 0.25390625}]}, "total": {"test_mcc": -0.685208890378868, "test_mcc_se": 1.8362297475873246, "test_accuracy": 25.039062499999996, "test_accuracy_se": 2.4543824736220134}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.08719555876819683, "accuracy": 0.265625}, {"mcc": -0.146596312255935, "accuracy": 0.2421875}, {"mcc": 0.006079086967007631, "accuracy": 0.3125}, {"mcc": -0.07444894451550219, "accuracy": 0.3203125}, {"mcc": 0.025096996765459356, "accuracy": 0.171875}, {"mcc": -0.06068131301070984, "accuracy": 0.1875}, {"mcc": 0.06854232442041162, "accuracy": 0.3984375}, {"mcc": -0.09008481160447392, "accuracy": 0.28125}, {"mcc": 0.06925842979160136, "accuracy": 0.390625}, {"mcc": -0.06635270537589283, "accuracy": 0.296875}]}, "total": {"test_mcc": -3.5638280758623067, "test_mcc_se": 4.540412397323967, "test_accuracy": 28.671875000000004, "test_accuracy_se": 4.6503972584563495}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.06526396427506816, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": -0.056854363636301815, "accuracy": 0.21875}, {"mcc": 0.07290316944026093, "accuracy": 0.23046875}, {"mcc": -2.1497285501961673e-05, "accuracy": 0.25}, {"mcc": 0.017523278269778154, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.02776335937414326, "accuracy": 0.23828125}, {"mcc": 0.07803657660623552, "accuracy": 0.27734375}]}, "total": {"test_mcc": 2.046144870436822, "test_mcc_se": 2.5959247882281122, "test_accuracy": 25.5078125, "test_accuracy_se": 1.4348519730138733}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 1967.42, "test_speed_short": 233.70000000000002}, {"test_speed": 3757.5600000000004, "test_speed_short": 436.80999999999995}, {"test_speed": 5124.61, "test_speed_short": 840.64}, {"test_speed": 6570.2, "test_speed_short": 1048.34}, {"test_speed": 7558.760000000001, "test_speed_short": 1236.9499999999998}, {"test_speed": 8769.61, "test_speed_short": 1608.19}, {"test_speed": 9944.56, "test_speed_short": 1822.8600000000001}, {"test_speed": 10973.62, "test_speed_short": 1976.52}, {"test_speed": 11808.16, "test_speed_short": 2194.0}, {"test_speed": 12677.07, "test_speed_short": 2426.34}]}, "total": {"test_speed": 7915.157000000001, "test_speed_se": 2209.9507654335057, "test_speed_short": 1382.435, "test_speed_short_se": 461.34668777594527}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}