
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.35518607021558046, "macro_f1": 0.43526041053416537}, {"mcc": 0.27918473011692274, "macro_f1": 0.37762957123088375}, {"mcc": 0.37229822078705205, "macro_f1": 0.5633428015916495}, {"mcc": 0.3366718281433893, "macro_f1": 0.40391143537518365}, {"mcc": 0.37457968221213434, "macro_f1": 0.44969135802469135}, {"mcc": 0.2097969770636337, "macro_f1": 0.355657962109575}, {"mcc": 0.30410057061104384, "macro_f1": 0.42119992342631196}, {"mcc": 0.3946382483636516, "macro_f1": 0.452192378328742}, {"mcc": 0.3457724202803098, "macro_f1": 0.43257534292017047}, {"mcc": 0.4318769956363507, "macro_f1": 0.4599063636905827}]}, "total": {"test_mcc": 34.04105743430069, "test_mcc_se": 3.9134105953203524, "test_macro_f1": 43.513675472319555, "test_macro_f1_se": 3.478829229847264}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.23856209150326796, "micro_f1": 0.21529745042492918}, {"micro_f1_no_misc": 0.14942528735632185, "micro_f1": 0.13666666666666666}, {"micro_f1_no_misc": 0.006802721088435374, "micro_f1": 0.03985507246376812}, {"micro_f1_no_misc": 0.1900647948164147, "micro_f1": 0.16544117647058823}, {"micro_f1_no_misc": 0.1254125412541254, "micro_f1": 0.12012012012012012}, {"micro_f1_no_misc": 0.14545454545454548, "micro_f1": 0.12751677852348994}, {"micro_f1_no_misc": 0.15827338129496404, "micro_f1": 0.1437699680511182}, {"micro_f1_no_misc": 0.11960132890365448, "micro_f1": 0.11680911680911683}, {"micro_f1_no_misc": 0.1757105943152455, "micro_f1": 0.1320450885668277}, {"micro_f1_no_misc": 0.12802768166089964, "micro_f1": 0.1394658753709199}]}, "total": {"test_micro_f1_no_misc": 14.373349676478744, "test_micro_f1_no_misc_se": 3.714421618176756, "test_micro_f1": 13.36987313467545, "test_micro_f1_se": 2.7037782498405227}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.030455672411361313, "macro_f1": 0.3801227083219546}, {"mcc": 0.0, "macro_f1": 0.46440013076168685}, {"mcc": -0.020828752216792853, "macro_f1": 0.36955724390882894}, {"mcc": -0.003944481036418488, "macro_f1": 0.4487823605014104}, {"mcc": 0.08561667029818171, "macro_f1": 0.3729241807626285}, {"mcc": -0.0872797785719991, "macro_f1": 0.34091396695987985}, {"mcc": -0.02430938013719691, "macro_f1": 0.419441744675772}, {"mcc": 0.051639777949432225, "macro_f1": 0.36318407960199}, {"mcc": 0.10685243369373369, "macro_f1": 0.41013824884792627}, {"mcc": -0.05588366139390376, "macro_f1": 0.3758722152164775}]}, "total": {"test_mcc": 0.8231850099639784, "test_mcc_se": 3.778352861821553, "test_macro_f1": 39.453368795585554, "test_macro_f1_se": 2.4613196189979685}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"em": 38.69047619047619, "f1": 42.465913715913715}, {"em": 40.0, "f1": 43.38385225885226}, {"em": 34.93975903614458, "f1": 38.97499087258123}, {"em": 36.30952380952381, "f1": 40.50075585789872}, {"em": 40.645161290322584, "f1": 45.5657962109575}, {"em": 37.64705882352941, "f1": 42.58349493643611}, {"em": 34.355828220858896, "f1": 37.88684389911384}, {"em": 26.70807453416149, "f1": 30.871461182020184}, {"em": 33.54430379746835, "f1": 38.600562587904356}, {"em": 35.15151515151515, "f1": 40.20446893174166}]}, "total": {"test_em": 35.799170085400036, "test_em_se": 2.480423066762068, "test_f1": 40.103814045341956, "test_f1_se": 2.497863701454097}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.14136913641191975, "macro_f1": 0.3114638737471493}, {"mcc": 0.0, "macro_f1": 0.14634146341463414}, {"mcc": 0.03534319192453339, "macro_f1": 0.22649616022192906}, {"mcc": 0.08559226427019176, "macro_f1": 0.21515953367080842}, {"mcc": 0.10359896155113996, "macro_f1": 0.2400141392718275}, {"mcc": 0.12125734759442537, "macro_f1": 0.3411540214171793}, {"mcc": 0.31160394248745316, "macro_f1": 0.3956073708341205}, {"mcc": 0.2536962249684162, "macro_f1": 0.4224724372507624}, {"mcc": 0.09017524047903357, "macro_f1": 0.22783389450056116}, {"mcc": 0.1729435045194054, "macro_f1": 0.41152443431925945}]}, "total": {"test_mcc": 13.155798142065183, "test_mcc_se": 5.858957513075483, "test_macro_f1": 29.380673286482317, "test_macro_f1_se": 5.953490466921334}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.02824858757062147, "micro_f1": 0.05614035087719299}, {"micro_f1_no_misc": 0.01038961038961039, "micro_f1": 0.01553398058252427}, {"micro_f1_no_misc": 0.08421052631578947, "micro_f1": 0.08805031446540881}, {"micro_f1_no_misc": 0.09130434782608694, "micro_f1": 0.08872458410351203}, {"micro_f1_no_misc": 0.12788632326820604, "micro_f1": 0.12520064205457465}, {"micro_f1_no_misc": 0.0915032679738562, "micro_f1": 0.13043478260869565}, {"micro_f1_no_misc": 0.07615230460921844, "micro_f1": 0.07704160246533127}, {"micro_f1_no_misc": 0.011080332409972301, "micro_f1": 0.1296551724137931}, {"micro_f1_no_misc": 0.053571428571428575, "micro_f1": 0.04245283018867924}, {"micro_f1_no_misc": 0.10993657505285413, "micro_f1": 0.11447811447811447}]}, "total": {"test_micro_f1_no_misc": 6.8428330398764405, "test_micro_f1_no_misc_se": 2.5400399213334115, "test_micro_f1": 8.677123742378265, "test_micro_f1_se": 2.4504149469700907}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"bertscore": 0.5768308055121452, "rouge_l": 0.11530059474170634}, {"bertscore": 0.5749611138598993, "rouge_l": 0.1167863073987071}, {"bertscore": 0.5708496569423005, "rouge_l": 0.09917787148676435}, {"bertscore": 0.628724463400431, "rouge_l": 0.14530517526538414}, {"bertscore": 0.5823411884484813, "rouge_l": 0.12025151260666536}, {"bertscore": 0.6220454624854028, "rouge_l": 0.14332318012335116}, {"bertscore": 0.5721241178689525, "rouge_l": 0.10384514701336606}, {"bertscore": 0.5979103631107137, "rouge_l": 0.12629304877231762}, {"bertscore": 0.597724124090746, "rouge_l": 0.13625657396434515}, {"bertscore": 0.6268335273489356, "rouge_l": 0.15114196714475564}]}, "total": {"test_bertscore": 59.50344823068008, "test_bertscore_se": 1.4457007860873563, "test_rouge_l": 12.576813785173629, "test_rouge_l_se": 1.1034034193295614}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.05115465928055023, "macro_f1": 0.44305590690614727}, {"mcc": -0.04706497621604712, "macro_f1": 0.3698017476275486}, {"mcc": 0.01800250112619768, "macro_f1": 0.36600922906978817}, {"mcc": 0.011380752593945711, "macro_f1": 0.48808867755192015}, {"mcc": 0.10321442750728609, "macro_f1": 0.4892328861369109}, {"mcc": 0.009665607189440706, "macro_f1": 0.48840927258193445}, {"mcc": -0.023417157276098777, "macro_f1": 0.47619047619047616}, {"mcc": -0.11514154661795957, "macro_f1": 0.4314314314314315}, {"mcc": -0.13026411630938972, "macro_f1": 0.393939393939394}, {"mcc": -0.016265001215808886, "macro_f1": 0.4898785425101214}]}, "total": {"test_mcc": -2.410441684989841, "test_mcc_se": 4.196677138723446, "test_macro_f1": 44.36037563945673, "test_macro_f1_se": 3.1573222611431686}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": -0.028384140682101006, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.3046875}, {"mcc": 0.10088634083907931, "accuracy": 0.3515625}, {"mcc": 0.0, "accuracy": 0.296875}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.0, "accuracy": 0.328125}, {"mcc": 0.0747959184681732, "accuracy": 0.328125}, {"mcc": 0.024919897531243386, "accuracy": 0.265625}]}, "total": {"test_mcc": 1.722180161563949, "test_mcc_se": 2.464828426118755, "test_accuracy": 29.921874999999996, "test_accuracy_se": 1.7980871515346213}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.03046480879906189, "accuracy": 0.3359375}, {"mcc": 0.12131496817443596, "accuracy": 0.4140625}, {"mcc": 0.11469464011865305, "accuracy": 0.3671875}, {"mcc": 0.04726310723560305, "accuracy": 0.3828125}, {"mcc": -0.08445739633771641, "accuracy": 0.296875}, {"mcc": 0.07878340693893382, "accuracy": 0.3515625}, {"mcc": -0.0938954146352917, "accuracy": 0.359375}, {"mcc": 0.08705309281450903, "accuracy": 0.3828125}, {"mcc": 0.029052065519573258, "accuracy": 0.2890625}, {"mcc": 0.08230413460886296, "accuracy": 0.390625}]}, "total": {"test_mcc": 4.1257741323662485, "test_mcc_se": 4.682378313165761, "test_accuracy": 35.703125, "test_accuracy_se": 2.4906032168293195}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"em": 21.428571428571427, "f1": 25.255372616883008}, {"em": 26.25, "f1": 29.32854278074867}, {"em": 23.49397590361446, "f1": 26.606694156304357}, {"em": 25.595238095238095, "f1": 28.546974345293673}, {"em": 25.806451612903224, "f1": 29.744542196155102}, {"em": 8.823529411764707, "f1": 10.980392156862743}, {"em": 22.085889570552148, "f1": 24.696144306393315}, {"em": 8.074534161490684, "f1": 10.23070888434425}, {"em": 20.88607594936709, "f1": 22.825387155245682}, {"em": 23.03030303030303, "f1": 27.059687786960517}]}, "total": {"test_em": 20.547456916380487, "test_em_se": 4.115402081298903, "test_f1": 23.527444638519135, "test_f1_se": 4.424062602425179}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.26171875}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": 0.03827476197880848, "accuracy": 0.23828125}]}, "total": {"test_mcc": 0.3827476197880848, "test_mcc_se": 0.7501853347846462, "test_accuracy": 25.234374999999996, "test_accuracy_se": 1.0092836215947647}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "AI-Sweden-Models/gpt-sw3-356m", "results": {"raw": {"test": [{"test_speed": 2189.46, "test_speed_short": 291.3}, {"test_speed": 3813.67, "test_speed_short": 405.65000000000003}, {"test_speed": 6571.75, "test_speed_short": 1128.8700000000001}, {"test_speed": 8154.99, "test_speed_short": 1388.28}, {"test_speed": 9714.539999999999, "test_speed_short": 1640.1}, {"test_speed": 11128.37, "test_speed_short": 2086.3399999999997}, {"test_speed": 10569.25, "test_speed_short": 1923.72}, {"test_speed": 13266.4, "test_speed_short": 2616.25}, {"test_speed": 15133.26, "test_speed_short": 2868.0}, {"test_speed": 15560.27, "test_speed_short": 3030.2000000000003}]}, "total": {"test_speed": 9610.196, "test_speed_se": 2791.2536696756856, "test_speed_short": 1737.8709999999999, "test_speed_short_se": 593.2346414490478}}, "num_model_parameters": 470607872, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.5760471230605617, "rouge_l": 0.09705440079024233}, {"bertscore": 0.5524321855045855, "rouge_l": 0.09902130289160324}, {"bertscore": 0.5577755308477208, "rouge_l": 0.07976092448376546}, {"bertscore": 0.4145923152100295, "rouge_l": 0.06110526104327817}, {"bertscore": 0.5287695411825553, "rouge_l": 0.08861783501955167}, {"bertscore": 0.5560010847402737, "rouge_l": 0.10270217779267202}, {"bertscore": 0.5504610406933352, "rouge_l": 0.08959344005072686}, {"bertscore": 0.5459430078044534, "rouge_l": 0.08978267818001737}, {"bertscore": 0.5001479594502598, "rouge_l": 0.09506851106101627}, {"bertscore": 0.48966777918394655, "rouge_l": 0.08889251091063122}]}, "total": {"test_bertscore": 52.71837567677722, "test_bertscore_se": 2.958300153307908, "test_rouge_l": 8.915990422235046, "test_rouge_l_se": 0.7316808404786563}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.02269037410444548, "accuracy": 0.23046875}, {"mcc": -0.04669612484854005, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": -0.0349143066109712, "accuracy": 0.25390625}, {"mcc": 0.04512502380174535, "accuracy": 0.21875}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": -0.005908121662595136, "accuracy": 0.265625}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": -0.04881773382197126, "accuracy": 0.25390625}]}, "total": {"test_mcc": -0.685208890378868, "test_mcc_se": 1.8362297475873246, "test_accuracy": 25.039062499999996, "test_accuracy_se": 2.4543824736220134}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.08719555876819683, "accuracy": 0.265625}, {"mcc": -0.146596312255935, "accuracy": 0.2421875}, {"mcc": 0.006079086967007631, "accuracy": 0.3125}, {"mcc": -0.07444894451550219, "accuracy": 0.3203125}, {"mcc": 0.025096996765459356, "accuracy": 0.171875}, {"mcc": -0.06068131301070984, "accuracy": 0.1875}, {"mcc": 0.06854232442041162, "accuracy": 0.3984375}, {"mcc": -0.09008481160447392, "accuracy": 0.28125}, {"mcc": 0.06925842979160136, "accuracy": 0.390625}, {"mcc": -0.06635270537589283, "accuracy": 0.296875}]}, "total": {"test_mcc": -3.5638280758623067, "test_mcc_se": 4.540412397323967, "test_accuracy": 28.671875000000004, "test_accuracy_se": 4.6503972584563495}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.06526396427506816, "accuracy": 0.28515625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25}, {"mcc": -0.056854363636301815, "accuracy": 0.21875}, {"mcc": 0.07290316944026093, "accuracy": 0.23046875}, {"mcc": -2.1497285501961673e-05, "accuracy": 0.25}, {"mcc": 0.017523278269778154, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.02776335937414326, "accuracy": 0.23828125}, {"mcc": 0.07803657660623552, "accuracy": 0.27734375}]}, "total": {"test_mcc": 2.046144870436822, "test_mcc_se": 2.5959247882281122, "test_accuracy": 25.5078125, "test_accuracy_se": 1.4348519730138733}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 1967.42, "test_speed_short": 233.70000000000002}, {"test_speed": 3757.5600000000004, "test_speed_short": 436.80999999999995}, {"test_speed": 5124.61, "test_speed_short": 840.64}, {"test_speed": 6570.2, "test_speed_short": 1048.34}, {"test_speed": 7558.760000000001, "test_speed_short": 1236.9499999999998}, {"test_speed": 8769.61, "test_speed_short": 1608.19}, {"test_speed": 9944.56, "test_speed_short": 1822.8600000000001}, {"test_speed": 10973.62, "test_speed_short": 1976.52}, {"test_speed": 11808.16, "test_speed_short": 2194.0}, {"test_speed": 12677.07, "test_speed_short": 2426.34}]}, "total": {"test_speed": 7915.157000000001, "test_speed_se": 2209.9507654335057, "test_speed_short": 1382.435, "test_speed_short_se": 461.34668777594527}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.10372195082903254, "macro_f1": 0.2891245111428598}, {"mcc": 0.24217248104679717, "macro_f1": 0.4299213991130437}, {"mcc": 0.03429813580051047, "macro_f1": 0.2318627450980392}, {"mcc": 0.07771750423344326, "macro_f1": 0.18345870374591597}, {"mcc": 0.201137479069156, "macro_f1": 0.34314466021783097}, {"mcc": 0.13318326686296464, "macro_f1": 0.33054954598677116}, {"mcc": 0.2788450006159256, "macro_f1": 0.49237102120007775}, {"mcc": 0.09873060357549425, "macro_f1": 0.3396184059329719}, {"mcc": 0.08797519751588977, "macro_f1": 0.23119868637110016}, {"mcc": 0.07909454841897283, "macro_f1": 0.30895132811300474}]}, "total": {"test_mcc": 13.368761679681866, "test_mcc_se": 4.958031902534744, "test_macro_f1": 31.802010069216152, "test_macro_f1_se": 5.775994206932882}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.14661654135338345, "micro_f1": 0.19834710743801653}, {"micro_f1_no_misc": 0.0049261083743842365, "micro_f1": 0.0861812778603269}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.04467353951890034}, {"micro_f1_no_misc": 0.06746987951807229, "micro_f1": 0.056451612903225805}, {"micro_f1_no_misc": 0.16216216216216214, "micro_f1": 0.14634146341463414}, {"micro_f1_no_misc": 0.18497109826589594, "micro_f1": 0.15136876006441222}, {"micro_f1_no_misc": 0.07766990291262135, "micro_f1": 0.06639004149377593}, {"micro_f1_no_misc": 0.08482142857142856, "micro_f1": 0.14804845222072677}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.1439509954058193}, {"micro_f1_no_misc": 0.10096153846153846, "micro_f1": 0.08984375}]}, "total": {"test_micro_f1_no_misc": 8.295986596194865, "test_micro_f1_no_misc_se": 4.192591373834753, "test_micro_f1": 11.315970003198379, "test_micro_f1_se": 3.156906128975784}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.03990596876004974, "macro_f1": 0.42329972108989483}, {"mcc": -0.04989615772799142, "macro_f1": 0.3644512668902913}, {"mcc": 0.0, "macro_f1": 0.3081081081081081}, {"mcc": 0.025352744740990613, "macro_f1": 0.5077453197347978}, {"mcc": 0.11585839547037943, "macro_f1": 0.5140859140859141}, {"mcc": -0.03970445566696172, "macro_f1": 0.42434684138154166}, {"mcc": 0.008931316628034687, "macro_f1": 0.4275885792913656}, {"mcc": 0.03941164060714018, "macro_f1": 0.5174030989930571}, {"mcc": -0.021408634078927936, "macro_f1": 0.3644512668902913}, {"mcc": 0.002672144834688232, "macro_f1": 0.3885565724672909}]}, "total": {"test_mcc": 0.4131102604730233, "test_mcc_se": 3.0400956960647214, "test_macro_f1": 42.400366889325525, "test_macro_f1_se": 4.414022130184473}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"em": 25.0, "f1": 28.455687830687825}, {"em": 26.875, "f1": 30.451388888888886}, {"em": 26.50602409638554, "f1": 31.033701907195873}, {"em": 29.166666666666668, "f1": 31.431878306878303}, {"em": 26.451612903225808, "f1": 29.569892473118276}, {"em": 11.176470588235293, "f1": 12.647058823529411}, {"em": 26.380368098159508, "f1": 31.509768044555024}, {"em": 18.012422360248447, "f1": 21.741102237996646}, {"em": 23.417721518987342, "f1": 26.464007160209693}, {"em": 24.848484848484848, "f1": 30.432086130087814}]}, "total": {"test_em": 23.783477108039346, "test_em_se": 3.297883585327548, "test_f1": 27.373657180314773, "test_f1_se": 3.702844098082199}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.5785667680902407, "rouge_l": 0.10478266684849649}, {"bertscore": 0.5366389472037554, "rouge_l": 0.0913718787126778}, {"bertscore": 0.5588699183426797, "rouge_l": 0.0592465602710938}, {"bertscore": 0.5264512610156089, "rouge_l": 0.10056308035402678}, {"bertscore": 0.5102915475727059, "rouge_l": 0.08544107986383509}, {"bertscore": 0.5299519307445735, "rouge_l": 0.0852840656238707}, {"bertscore": 0.5573177034966648, "rouge_l": 0.08020972095937073}, {"bertscore": 0.5413106380729005, "rouge_l": 0.09593393798762267}, {"bertscore": 0.5201394803007133, "rouge_l": 0.10335046660648749}, {"bertscore": 0.5671830219216645, "rouge_l": 0.10445701291820447}]}, "total": {"test_bertscore": 54.26721216761508, "test_bertscore_se": 1.3681781949241512, "test_rouge_l": 9.10640470145686, "test_rouge_l_se": 0.8832731310629042}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.028692677432506374, "accuracy": 0.28125}, {"mcc": -0.0024208992722143754, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.3046875}, {"mcc": 0.03698554815658994, "accuracy": 0.34375}, {"mcc": -0.024678211064752303, "accuracy": 0.29296875}, {"mcc": 0.0, "accuracy": 0.2890625}, {"mcc": 0.025403295655385315, "accuracy": 0.33203125}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": -0.0008050402599027842, "accuracy": 0.25}]}, "total": {"test_mcc": 0.6317737064761216, "test_mcc_se": 1.1415423327908454, "test_accuracy": 29.531249999999996, "test_accuracy_se": 1.768871768972936}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.027267737597966975, "accuracy": 0.2421875}, {"mcc": -0.043750604530014926, "accuracy": 0.2734375}, {"mcc": 0.10755559175168214, "accuracy": 0.359375}, {"mcc": 0.0, "accuracy": 0.359375}, {"mcc": 0.09621545355176914, "accuracy": 0.3125}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.09214635435825942, "accuracy": 0.28125}, {"mcc": 0.11471021025308402, "accuracy": 0.375}, {"mcc": 0.139565504088289, "accuracy": 0.359375}, {"mcc": 0.005244787305023077, "accuracy": 0.359375}]}, "total": {"test_mcc": 4.84419559180125, "test_mcc_se": 4.192163287920145, "test_accuracy": 31.953125, "test_accuracy_se": 3.006263174203058}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "angry-tweets", "task": "sentiment-classification", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.0, "macro_f1": 0.13991769547325103}, {"mcc": 0.0, "macro_f1": 0.16023738872403562}, {"mcc": 0.024774418294401634, "macro_f1": 0.2055286717703186}, {"mcc": 0.0, "macro_f1": 0.15568862275449102}, {"mcc": 0.0, "macro_f1": 0.17198067632850242}, {"mcc": 0.0, "macro_f1": 0.13664596273291926}, {"mcc": 0.0, "macro_f1": 0.13664596273291926}, {"mcc": 0.0, "macro_f1": 0.15415415415415415}, {"mcc": 0.028705520784502433, "macro_f1": 0.21596582163164013}, {"mcc": 0.0, "macro_f1": 0.13499480789200416}]}, "total": {"test_mcc": 0.5347993907890407, "test_mcc_se": 0.7011604066922986, "test_macro_f1": 16.11759764194236, "test_macro_f1_se": 1.7900545841413757}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.03378659359090625, "accuracy": 0.25}, {"mcc": 0.0, "accuracy": 0.27734375}, {"mcc": 0.10542575249516412, "accuracy": 0.296875}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": -0.07154608030212854, "accuracy": 0.22265625}, {"mcc": 0.029984866336036215, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": -0.019647296692061614, "accuracy": 0.2265625}]}, "total": {"test_mcc": 0.10430648246103925, "test_mcc_se": 2.8214011787979816, "test_accuracy": 24.9609375, "test_accuracy_se": 1.4546874999999997}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "dansk", "task": "named-entity-recognition", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"micro_f1_no_misc": 0.02257336343115124, "micro_f1": 0.044293015332197615}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.015037593984962405}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.005291005291005291, "micro_f1": 0.0045662100456621}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.0}, {"micro_f1_no_misc": 0.0, "micro_f1": 0.019138755980861243}, {"micro_f1_no_misc": 0.041176470588235294, "micro_f1": 0.04109589041095891}]}, "total": {"test_micro_f1_no_misc": 0.6904083931039182, "test_micro_f1_no_misc_se": 0.8658551154205193, "test_micro_f1": 1.241314657546423, "test_micro_f1_se": 1.0781292382920409}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_is_da_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 1826.3700000000001, "test_speed_short": 213.5}, {"test_speed": 3574.75, "test_speed_short": 422.75}, {"test_speed": 4886.13, "test_speed_short": 775.89}, {"test_speed": 6331.94, "test_speed_short": 939.78}, {"test_speed": 7387.379999999999, "test_speed_short": 1147.8500000000001}, {"test_speed": 8499.11, "test_speed_short": 1533.0}, {"test_speed": 9155.81, "test_speed_short": 1674.44}, {"test_speed": 10353.56, "test_speed_short": 1827.2799999999997}, {"test_speed": 11329.67, "test_speed_short": 2032.0}, {"test_speed": 11478.74, "test_speed_short": 2237.77}]}, "total": {"test_speed": 7482.3460000000005, "test_speed_se": 2047.1696063467464, "test_speed_short": 1280.426, "test_speed_short_se": 426.5007261138878}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scala-da", "task": "linguistic-acceptability", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.05597892954492245, "macro_f1": 0.4395583476108891}, {"mcc": 0.07882634225314346, "macro_f1": 0.4832806297517325}, {"mcc": -0.01616335513027424, "macro_f1": 0.36125710038753517}, {"mcc": -0.0912086009435502, "macro_f1": 0.33726520681265204}, {"mcc": 0.060057729003332144, "macro_f1": 0.35113468906572354}, {"mcc": -0.02887298234528568, "macro_f1": 0.36790123456790125}, {"mcc": -0.027187940356231143, "macro_f1": 0.3491235113073732}, {"mcc": 0.08896008544727063, "macro_f1": 0.39806159316396167}, {"mcc": 0.037004898065283946, "macro_f1": 0.515621394619041}, {"mcc": 0.0, "macro_f1": 0.36}]}, "total": {"test_mcc": 0.4543724644876646, "test_mcc_se": 3.7064253956158875, "test_macro_f1": 39.632037072868094, "test_macro_f1_se": 3.8530289303139154}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "scandiqa-da", "task": "reading-comprehension", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"em": 11.904761904761905, "f1": 16.909921584263685}, {"em": 18.75, "f1": 23.64710576259489}, {"em": 21.08433734939759, "f1": 25.977137477846192}, {"em": 16.071428571428573, "f1": 20.301174614531536}, {"em": 12.903225806451612, "f1": 17.918193986105702}, {"em": 18.823529411764707, "f1": 21.844948080242197}, {"em": 16.56441717791411, "f1": 20.80371154603334}, {"em": 9.937888198757763, "f1": 13.281497315658806}, {"em": 19.620253164556964, "f1": 23.489388931839066}, {"em": 10.303030303030303, "f1": 14.470826140972616}]}, "total": {"test_em": 15.596287188806354, "test_em_se": 2.519792535970748, "test_f1": 19.864390544008806, "test_f1_se": 2.570121686826835}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "nordjylland-news", "task": "summarization", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"bertscore": 0.48654443357372656, "rouge_l": 0.0770054255941777}, {"bertscore": 0.5089967101812363, "rouge_l": 0.08276018968503773}, {"bertscore": 0.48248690320178866, "rouge_l": 0.05913648107142949}, {"bertscore": 0.48192859173286706, "rouge_l": 0.07862994262793081}, {"bertscore": 0.5095930031966418, "rouge_l": 0.07775039913001365}, {"bertscore": 0.4682572314632125, "rouge_l": 0.05703117520175616}, {"bertscore": 0.5032303194748238, "rouge_l": 0.07904826545370841}, {"bertscore": 0.4719089058926329, "rouge_l": 0.06474196973565394}, {"bertscore": 0.4825666706310585, "rouge_l": 0.08366849513293367}, {"bertscore": 0.5271260933950543, "rouge_l": 0.06574575783356387}]}, "total": {"test_bertscore": 49.226388627430424, "test_bertscore_se": 1.1752234338625906, "test_rouge_l": 7.255181014662053, "test_rouge_l_se": 0.6135688651597584}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danske-talemaader", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.017263422534464553, "accuracy": 0.24609375}, {"mcc": 0.13036318798426683, "accuracy": 0.3359375}, {"mcc": 0.0, "accuracy": 0.16796875}, {"mcc": 0.005009903345364105, "accuracy": 0.3046875}, {"mcc": 0.045982347726996234, "accuracy": 0.234375}, {"mcc": -0.06858022317765142, "accuracy": 0.2265625}, {"mcc": -0.001138877374839713, "accuracy": 0.2890625}, {"mcc": 0.07379717512888416, "accuracy": 0.3046875}, {"mcc": 0.0, "accuracy": 0.3125}, {"mcc": 0.03339873925454003, "accuracy": 0.26953125}]}, "total": {"test_mcc": 2.0156883035309567, "test_mcc_se": 3.37180171755524, "test_accuracy": 26.914062500000004, "test_accuracy_se": 3.136986949323458}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "danish-citizen-tests", "task": "knowledge", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": -0.09180900593842722, "accuracy": 0.2265625}, {"mcc": -0.052108752337854884, "accuracy": 0.265625}, {"mcc": 0.1524065829467407, "accuracy": 0.375}, {"mcc": 0.0, "accuracy": 0.359375}, {"mcc": -0.08819548466396175, "accuracy": 0.2578125}, {"mcc": 0.0, "accuracy": 0.2734375}, {"mcc": 0.0, "accuracy": 0.2421875}, {"mcc": -0.08969724880451486, "accuracy": 0.2890625}, {"mcc": 0.0315501684982055, "accuracy": 0.2421875}, {"mcc": 0.00682581803109215, "accuracy": 0.359375}]}, "total": {"test_mcc": -1.310279222687204, "test_mcc_se": 4.597591450111877, "test_accuracy": 28.90625, "test_accuracy_se": 3.4163617750141415}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "hellaswag-da", "task": "common-sense-reasoning", "dataset_languages": ["da"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"mcc": 0.05076563314591289, "accuracy": 0.265625}, {"mcc": -0.01404065858358923, "accuracy": 0.26171875}, {"mcc": 0.025769707038786015, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.24609375}, {"mcc": 0.006404104064463259, "accuracy": 0.2734375}, {"mcc": 0.002906107294259899, "accuracy": 0.25390625}, {"mcc": 0.0, "accuracy": 0.25390625}, {"mcc": -0.0042522685170867675, "accuracy": 0.23046875}, {"mcc": 0.0, "accuracy": 0.234375}, {"mcc": -0.021148034671053955, "accuracy": 0.2265625}]}, "total": {"test_mcc": 0.4640458977169211, "test_mcc_se": 1.2631162132729876, "test_accuracy": 25.0, "test_accuracy_se": 0.9616986879056478}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}
{"dataset": "speed", "task": "speed", "dataset_languages": ["ab", "aa", "af", "sq", "am", "ar", "an", "hy", "as", "av", "ae", "ay", "az", "bm", "ba", "eu", "be", "bn", "bi", "bs", "br", "bg", "my", "ca", "ch", "ce", "ny", "zh", "cu", "cv", "kw", "co", "cr", "hr", "cs", "da", "dv", "nl", "dz", "en", "eo", "et", "ee", "fo", "fj", "fi", "fr", "fy", "ff", "gd", "gl", "lg", "ka", "de", "el", "kl", "gn", "gu", "ht", "ha", "he", "hz", "hi", "ho", "hu", "is", "io", "ig", "id", "ia", "ie", "iu", "ik", "ga", "it", "ja", "kn", "kr", "ks", "kk", "km", "ki", "rw", "ky", "kv", "kg", "ko", "kj", "ku", "lo", "la", "lv", "li", "ln", "lt", "lu", "lb", "mk", "mg", "ms", "ml", "mt", "gv", "mi", "mr", "mh", "mn", "na", "nv", "nd", "nr", "ng", "ne", "no", "nb", "nn", "ii", "oc", "oj", "or", "om", "os", "pi", "ps", "fa", "pl", "pt", "pa", "qu", "ro", "rm", "rn", "ru", "se", "sm", "sg", "sa", "sc", "sr", "sn", "sd", "si", "sk", "sl", "so", "st", "es", "su", "sw", "ss", "sv", "tl", "ty", "tg", "ta", "tt", "te", "th", "bo", "ti", "to", "ts", "tn", "tr", "tk", "tw", "ug", "uk", "ur", "uz", "ve", "vi", "vo", "wa", "cy", "wo", "xh", "yi", "yo", "za", "zu"], "model": "gpt_gptsw3_en_da_is_356m_gbs1024", "results": {"raw": {"test": [{"test_speed": 2010.19, "test_speed_short": 238.2}, {"test_speed": 3715.9300000000003, "test_speed_short": 429.59}, {"test_speed": 5170.679999999999, "test_speed_short": 847.67}, {"test_speed": 6577.419999999999, "test_speed_short": 1054.78}, {"test_speed": 7689.55, "test_speed_short": 1262.8}, {"test_speed": 8958.96, "test_speed_short": 1616.9499999999998}, {"test_speed": 10102.310000000001, "test_speed_short": 1811.3799999999999}, {"test_speed": 11016.88, "test_speed_short": 1978.34}, {"test_speed": 11929.810000000001, "test_speed_short": 2161.0}, {"test_speed": 12379.74, "test_speed_short": 2401.27}]}, "total": {"test_speed": 7955.147, "test_speed_se": 2200.1191057895467, "test_speed_short": 1380.1979999999999, "test_speed_short_se": 455.6063603952236}}, "num_model_parameters": -1, "max_sequence_length": 2048, "vocabulary_size": 64000, "generative": true, "few_shot": true, "validation_split": true, "scandeval_version": "13.2.0"}